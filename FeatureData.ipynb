{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, logging, re, nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import feature_extraction\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "class FeatureData(object):\n",
    "    def __init__(self, article_file_path, stances_file_path):\n",
    "        self.number_of_classes = 4\n",
    "        self.classes = ['agree', 'disagree', 'discuss', 'unrelated']\n",
    "        self.articles = self._get_articles(article_file_path)  # list of dictionaries\n",
    "        self.stances = self._get_stances(stances_file_path)\n",
    "        self.number_of_stances = len(self.stances)\n",
    "        self.number_of_articles = len(self.articles)\n",
    "\n",
    "    def get_clean_articles(self):\n",
    "        \"\"\"Returns a dictionary with Body ID's as keys and article bodies as values.\"\"\"\n",
    "        clean_articles = []\n",
    "        print('Retrieving clean articles...')\n",
    "\n",
    "        for item in tqdm.tqdm(self.articles):\n",
    "            cleaned_article = clean(item['articleBody'])\n",
    "            tokens = tokenize_text(cleaned_article)\n",
    "            no_stop_word_tokens = remove_stopwords(tokens)\n",
    "            lemmatized_tokens = get_tokenized_lemmas(no_stop_word_tokens)\n",
    "            clean_articles.append({'articleBody': ' '.join(lemmatized_tokens),\n",
    "                                   'Body ID': item['Body ID']})\n",
    "        return {article['Body ID']: article['articleBody'] for article in clean_articles}\n",
    "\n",
    "    #We need the stop words for POS tagging to work propperly\n",
    "    def get_original_articles(self):\n",
    "        clean_articles = []\n",
    "        print('Retrieving original articles...')\n",
    "        for item in tqdm.tqdm(self.articles):\n",
    "            #cleaned_article = clean(item['articleBody'])\n",
    "            cleaned_article = item['articleBody'].encode('ascii', 'ignore')\n",
    "            clean_articles.append({'articleBody':cleaned_article,\n",
    "                                   'Body ID': item['Body ID']})\n",
    "        return {article['Body ID']: article['articleBody'] for article in clean_articles}\n",
    "\n",
    "    def get_clean_stances(self):\n",
    "        \"\"\"Retrieves a list of dictionaries containing the fully cleaned Headlines and the Body ID and Stance for\n",
    "        each headline.\"\"\"\n",
    "        clean_headlines = []\n",
    "        print('Retrieving clean stances...')\n",
    "\n",
    "        for item in tqdm.tqdm(self.stances):\n",
    "            cleaned_headline = clean(item['Headline'])\n",
    "            tokens = tokenize_text(cleaned_headline)\n",
    "            no_stop_word_tokens = remove_stopwords(tokens)\n",
    "            lemmatized_tokens = get_tokenized_lemmas(no_stop_word_tokens)\n",
    "            clean_headlines.append({'Headline': ' '.join(lemmatized_tokens),\n",
    "                                    'originalHeadline': cleaned_headline,\n",
    "                                    'Body ID': item['Body ID'],\n",
    "                                    'Stance': item['Stance']})\n",
    "\n",
    "        return clean_headlines\n",
    "\n",
    "    def _get_articles(self, path):\n",
    "        # Body ID, articleBody\n",
    "        articles = []\n",
    "        dt=pd.read_csv(path)\n",
    "        articles=dt.T.to_dict().values()\n",
    "        return articles\n",
    "\n",
    "    def _get_stances(self, path):\n",
    "        # Headline, Body ID, Stance\n",
    "        stances = []\n",
    "        dt1=pd.read_csv(path)\n",
    "        stances=dt1.T.to_dict().values()\n",
    "        return stances\n",
    "\n",
    "\n",
    "def normalize_word(w):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(w).lower()\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    return \" \".join(re.findall(r'\\w+', text, flags=re.UNICODE)).lower()\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return [token for token in word_tokenize(text)]\n",
    "\n",
    "\n",
    "def remove_stopwords(list_of_tokens):\n",
    "    return [word for word in list_of_tokens if word not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
    "\n",
    "\n",
    "def get_tokenized_lemmas(tokens):\n",
    "    return [normalize_word(token) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = FeatureData('C:/Users/binni/Downloads/NLP-Fake-News-Challenge-master/NLP-Fake-News-Challenge-master/data/competition_test_bodies.csv', 'C:/Users/binni/Downloads/NLP-Fake-News-Challenge-master/NLP-Fake-News-Challenge-master/data/competition_test_stances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\binni\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "from gensim.models.phrases import Phraser\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_max_ngram_size=3\n",
    "_refuting_words = ['fake', 'fraud', 'hoax', 'false', 'deny', 'denies', 'not', 'despite', 'nope', 'doubt','doubts', 'bogus', 'debunk', 'pranks', 'retract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving clean stances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 25413/25413 [00:52<00:00, 488.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving clean articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 904/904 [00:19<00:00, 46.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving original articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 904/904 [00:00<00:00, 25111.43it/s]\n"
     ]
    }
   ],
   "source": [
    "_stances=feature_data.get_clean_stances()\n",
    "_articles=feature_data.get_clean_articles()\n",
    "_original_articles=feature_data.get_original_articles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                | 0/25413 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stance_vectorizer [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "vocab ['bag', 'bag round', 'bag round car', 'bean', 'bean bag', 'bean bag round', 'car', 'car window', 'cop', 'cop bean', 'cop bean bag', 'eye', 'eye cop', 'eye cop bean', 'ferguson', 'ferguson riot', 'ferguson riot pregnant', 'loses', 'loses eye', 'loses eye cop', 'pregnant', 'pregnant woman', 'pregnant woman loses', 'riot', 'riot pregnant', 'riot pregnant woman', 'round', 'round car', 'round car window', 'window', 'woman', 'woman loses', 'woman loses eye']\n",
      "ngram_counts [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "features ['bag', 'bag round', 'bag round car', 'bean', 'bean bag', 'bean bag round', 'car', 'car window', 'cop', 'cop bean', 'cop bean bag', 'eye', 'eye cop', 'eye cop bean', 'ferguson', 'ferguson riot', 'ferguson riot pregnant', 'loses', 'loses eye', 'loses eye cop', 'pregnant', 'pregnant woman', 'pregnant woman loses', 'riot', 'riot pregnant', 'riot pregnant woman', 'round', 'round car', 'round car window', 'window', 'woman', 'woman loses', 'woman loses eye']\n",
      "aggregated_counts [0, 0, 0]\n",
      "ngrams [[0.0, 0.0, 0.0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                        | 1/25413 [00:00<47:00,  9.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stance_vectorizer [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "vocab ['conservative', 'conservative sure', 'conservative sure gitmo', 'crazy', 'crazy conservative', 'crazy conservative sure', 'detainee', 'detainee killed', 'detainee killed james', 'foley', 'gitmo', 'gitmo detainee', 'gitmo detainee killed', 'james', 'james foley', 'killed', 'killed james', 'killed james foley', 'sure', 'sure gitmo', 'sure gitmo detainee']\n",
      "ngram_counts [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "features ['conservative', 'conservative sure', 'conservative sure gitmo', 'crazy', 'crazy conservative', 'crazy conservative sure', 'detainee', 'detainee killed', 'detainee killed james', 'foley', 'gitmo', 'gitmo detainee', 'gitmo detainee killed', 'james', 'james foley', 'killed', 'killed james', 'killed james foley', 'sure', 'sure gitmo', 'sure gitmo detainee']\n",
      "aggregated_counts [0, 0, 0]\n",
      "ngrams [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                        | 2/25413 [00:00<46:22,  9.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stance_vectorizer [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "vocab ['attack', 'bear', 'bear attack', 'bieber', 'bieber ringtone', 'bieber ringtone saved', 'guy', 'guy say', 'guy say justin', 'justin', 'justin bieber', 'justin bieber ringtone', 'ringtone', 'ringtone saved', 'ringtone saved bear', 'russian', 'russian guy', 'russian guy say', 'saved', 'saved bear', 'saved bear attack', 'say', 'say justin', 'say justin bieber']\n",
      "ngram_counts [[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "features ['attack', 'bear', 'bear attack', 'bieber', 'bieber ringtone', 'bieber ringtone saved', 'guy', 'guy say', 'guy say justin', 'justin', 'justin bieber', 'justin bieber ringtone', 'ringtone', 'ringtone saved', 'ringtone saved bear', 'russian', 'russian guy', 'russian guy say', 'saved', 'saved bear', 'saved bear attack', 'say', 'say justin', 'say justin bieber']\n",
      "aggregated_counts [1, 0, 0]\n",
      "ngrams [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.1111111111111111, 0.0, 0.0]]\n",
      "Stance_vectorizer [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "vocab ['believed', 'believed dead', 'believed dead meow', 'buried', 'buried kitty', 'buried kitty believed', 'cat', 'cat buried', 'cat buried kitty', 'dead', 'dead meow', 'dead meow life', 'kitty', 'kitty believed', 'kitty believed dead', 'life', 'meow', 'meow life', 'zombie', 'zombie cat', 'zombie cat buried']\n",
      "ngram_counts [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "features ['believed', 'believed dead', 'believed dead meow', 'buried', 'buried kitty', 'buried kitty believed', 'cat', 'cat buried', 'cat buried kitty', 'dead', 'dead meow', 'dead meow life', 'kitty', 'kitty believed', 'kitty believed dead', 'life', 'meow', 'meow life', 'zombie', 'zombie cat', 'zombie cat buried']\n",
      "aggregated_counts [0, 0, 0]\n",
      "ngrams [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.1111111111111111, 0.0, 0.0], [0.0, 0.0, 0.0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ngrams = []\n",
    "i=0\n",
    "for stance in tqdm.tqdm(_stances):\n",
    "    stance_vectorizer = CountVectorizer(input=stance['Headline'], ngram_range=(1, _max_ngram_size),binary=True)\n",
    "    a=stance_vectorizer.fit_transform([stance['Headline']]).toarray()\n",
    "    print (\"Stance_vectorizer\",a)\n",
    "    vocab = stance_vectorizer.get_feature_names()\n",
    "    print(\"vocab\",vocab)\n",
    "    vectorizer = CountVectorizer(input=_articles[stance['Body ID']], vocabulary=vocab,ngram_range=(1, _max_ngram_size))\n",
    "    ngram_counts = vectorizer.fit_transform([_articles[stance['Body ID']]]).toarray()\n",
    "    print(\"ngram_counts\",ngram_counts)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    print(\"features\",features)\n",
    "    aggregated_counts = [0 for _ in range(_max_ngram_size)]\n",
    "    for index in np.nditer(np.nonzero(ngram_counts[0]), ['zerosize_ok']):\n",
    "        aggregated_counts[len(features[index].split()) - 1] += ngram_counts[0][index]\n",
    "    print(\"aggregated_counts\",aggregated_counts)\n",
    "    standardized_counts = [1.0*count/len(stance['Headline'].split()) for count in aggregated_counts]\n",
    "    ngrams.append(standardized_counts)\n",
    "    print(\"ngrams\",ngrams)\n",
    "    i=i+1\n",
    "    if (i==4):\n",
    "        break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_original_articles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=[]\n",
    "if True:\n",
    "    print('Retrieving headline ngrams...')\n",
    "    ngrams = np.array(ngrams)\n",
    "    features.append(ngrams)\n",
    "    ngram_headings = [('ngram_' + str(count)) for count in range(1,_max_ngram_size + 1)]\n",
    "    feature_names.append(ngram_headings)\n",
    "    print(feature_names)\n",
    "    print(ngram_headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 25413/25413 [05:08<00:00, 82.29it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "all_words = []; atricle_words = []\n",
    "for stance in tqdm.tqdm(_stances):\n",
    "    if stance['Stance'] == 'unrelated':\n",
    "        pass\n",
    "    body_words = []; headline_words = []\n",
    "    headline = tokenizer.tokenize(stance['originalHeadline'])\n",
    "    body = (tokenizer.tokenize(_original_articles[stance['Body ID']].decode('utf-8')))[:4]\n",
    "    for s in headline:\n",
    "        s = word_tokenize(s)\n",
    "        headline_words = headline_words + s\n",
    "        all_words.append(s)\n",
    "    for s in body:\n",
    "        s = word_tokenize(s)\n",
    "        body_words = body_words + s\n",
    "        all_words.append(s)\n",
    "    atricle_words.append([headline_words, body_words])\n",
    "\n",
    "model = models.Word2Vec(all_words, size=100, min_count=1)\n",
    "cosine_similarities = []\n",
    "        # Generate sentence vectors and computer cosine similarity\n",
    "for headline, body in atricle_words:\n",
    "    h_vector = sum([model.wv[word] for word in headline])\n",
    "    b_vector = sum([model.wv[word] for word in body])\n",
    "    cosine_similarities.append(cosine_similarity(h_vector.reshape(1,-1), b_vector.reshape(1,-1)))\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 25413/25413 [00:00<00:00, 55447.77it/s]\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "i=0\n",
    "for stance in tqdm.tqdm(_stances):\n",
    "    count = [1 if refute_word in stance['Headline'] else 0 for refute_word in _refuting_words]\n",
    "    features.append(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 25413/25413 [03:46<00:00, 53.81it/s]\n"
     ]
    }
   ],
   "source": [
    "_refuting_words = ['fake', 'fraud', 'hoax', 'false', 'deny', 'denies', 'not','despite', 'nope', 'nowhere', 'doubt', 'doubts', 'bogus', 'debunk', 'pranks','retract', 'nothing', 'never', 'none', 'budge']\n",
    "\n",
    "def determine_polarity(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    return sum([token in _refuting_words for token in tokens]) % 2\n",
    "\n",
    "polarities = []\n",
    "for stance in tqdm.tqdm(_stances):\n",
    "    headline_polarity = determine_polarity(stance['Headline'])\n",
    "\n",
    "    body_polarity = determine_polarity(_articles.get(stance['Body ID']))\n",
    "\n",
    "    polarities.append([headline_polarity, body_polarity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                | 0/25413 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags ['NN']\n",
      "(2, 25)\n",
      "tfid   (0, 20)\t0.3535533905932738\n",
      "  (0, 23)\t0.3535533905932738\n",
      "  (0, 6)\t0.3535533905932738\n",
      "  (0, 5)\t0.3535533905932738\n",
      "  (0, 1)\t0.3535533905932738\n",
      "  (0, 21)\t0.3535533905932738\n",
      "  (0, 2)\t0.3535533905932738\n",
      "  (0, 22)\t0.3535533905932738\n",
      "  (1, 19)\t0.20851441405707477\n",
      "  (1, 18)\t0.41702882811414954\n",
      "  (1, 17)\t0.41702882811414954\n",
      "  (1, 3)\t0.20851441405707477\n",
      "  (1, 9)\t0.20851441405707477\n",
      "  (1, 15)\t0.20851441405707477\n",
      "  (1, 16)\t0.20851441405707477\n",
      "  (1, 13)\t0.20851441405707477\n",
      "  (1, 12)\t0.20851441405707477\n",
      "  (1, 11)\t0.20851441405707477\n",
      "  (1, 0)\t0.20851441405707477\n",
      "  (1, 24)\t0.20851441405707477\n",
      "  (1, 4)\t0.20851441405707477\n",
      "  (1, 10)\t0.20851441405707477\n",
      "  (1, 7)\t0.20851441405707477\n",
      "  (1, 8)\t0.20851441405707477\n",
      "  (1, 14)\t0.20851441405707477\n",
      "cosine [[1.0000000000000002, 0.0], [0.0, 0.9999999999999998]]\n",
      "stance_cosine [0.0]\n",
      "named_cosine [[0.0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                        | 1/25413 [00:00<51:00,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags ['NN']\n",
      "(2, 15)\n",
      "tfid   (0, 3)\t0.5773502691896257\n",
      "  (0, 5)\t0.5773502691896257\n",
      "  (0, 9)\t0.5773502691896257\n",
      "  (1, 4)\t0.17149858514250882\n",
      "  (1, 11)\t0.17149858514250882\n",
      "  (1, 12)\t0.34299717028501764\n",
      "  (1, 2)\t0.17149858514250882\n",
      "  (1, 13)\t0.5144957554275265\n",
      "  (1, 1)\t0.5144957554275265\n",
      "  (1, 8)\t0.34299717028501764\n",
      "  (1, 0)\t0.17149858514250882\n",
      "  (1, 10)\t0.17149858514250882\n",
      "  (1, 6)\t0.17149858514250882\n",
      "  (1, 14)\t0.17149858514250882\n",
      "  (1, 7)\t0.17149858514250882\n",
      "cosine [[1.0, 0.0], [0.0, 0.9999999999999999]]\n",
      "stance_cosine [0.0]\n",
      "named_cosine [[0.0], [0.0]]\n",
      "tags ['NN']\n",
      "(2, 17)\n",
      "tfid   (0, 8)\t0.5\n",
      "  (0, 9)\t0.5\n",
      "  (0, 14)\t0.5\n",
      "  (0, 2)\t0.5\n",
      "  (1, 0)\t0.21320071635561044\n",
      "  (1, 12)\t0.21320071635561044\n",
      "  (1, 16)\t0.4264014327112209\n",
      "  (1, 15)\t0.21320071635561044\n",
      "  (1, 1)\t0.21320071635561044\n",
      "  (1, 13)\t0.21320071635561044\n",
      "  (1, 6)\t0.4264014327112209\n",
      "  (1, 5)\t0.21320071635561044\n",
      "  (1, 10)\t0.21320071635561044\n",
      "  (1, 7)\t0.21320071635561044\n",
      "  (1, 4)\t0.21320071635561044\n",
      "  (1, 11)\t0.21320071635561044\n",
      "  (1, 3)\t0.4264014327112209\n",
      "cosine [[1.0, 0.0], [0.0, 0.9999999999999998]]\n",
      "stance_cosine [0.0]\n",
      "named_cosine [[0.0], [0.0], [0.0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                        | 3/25413 [00:00<48:54,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags ['NN']\n",
      "(2, 20)\n",
      "tfid   (0, 19)\t0.5\n",
      "  (0, 1)\t0.5\n",
      "  (0, 9)\t0.5\n",
      "  (0, 7)\t0.5\n",
      "  (1, 5)\t0.25000000000000006\n",
      "  (1, 10)\t0.25000000000000006\n",
      "  (1, 13)\t0.25000000000000006\n",
      "  (1, 17)\t0.25000000000000006\n",
      "  (1, 6)\t0.25000000000000006\n",
      "  (1, 16)\t0.25000000000000006\n",
      "  (1, 11)\t0.25000000000000006\n",
      "  (1, 12)\t0.25000000000000006\n",
      "  (1, 3)\t0.25000000000000006\n",
      "  (1, 4)\t0.25000000000000006\n",
      "  (1, 14)\t0.25000000000000006\n",
      "  (1, 0)\t0.25000000000000006\n",
      "  (1, 15)\t0.25000000000000006\n",
      "  (1, 2)\t0.25000000000000006\n",
      "  (1, 8)\t0.25000000000000006\n",
      "  (1, 18)\t0.25000000000000006\n",
      "cosine [[1.0, 0.0], [0.0, 1.0000000000000002]]\n",
      "stance_cosine [0.0]\n",
      "named_cosine [[0.0], [0.0], [0.0], [0.0]]\n",
      "tags ['NN']\n",
      "(2, 24)\n",
      "tfid   (0, 19)\t0.5\n",
      "  (0, 0)\t0.5\n",
      "  (0, 21)\t0.5\n",
      "  (0, 6)\t0.5\n",
      "  (1, 1)\t0.39223227027636803\n",
      "  (1, 16)\t0.19611613513818402\n",
      "  (1, 8)\t0.19611613513818402\n",
      "  (1, 17)\t0.19611613513818402\n",
      "  (1, 12)\t0.19611613513818402\n",
      "  (1, 11)\t0.19611613513818402\n",
      "  (1, 2)\t0.19611613513818402\n",
      "  (1, 5)\t0.39223227027636803\n",
      "  (1, 9)\t0.19611613513818402\n",
      "  (1, 22)\t0.19611613513818402\n",
      "  (1, 14)\t0.19611613513818402\n",
      "  (1, 15)\t0.19611613513818402\n",
      "  (1, 23)\t0.19611613513818402\n",
      "  (1, 18)\t0.19611613513818402\n",
      "  (1, 20)\t0.19611613513818402\n",
      "  (1, 13)\t0.19611613513818402\n",
      "  (1, 4)\t0.19611613513818402\n",
      "  (1, 3)\t0.19611613513818402\n",
      "  (1, 10)\t0.19611613513818402\n",
      "  (1, 7)\t0.19611613513818402\n",
      "cosine [[1.0, 0.0], [0.0, 0.9999999999999996]]\n",
      "stance_cosine [0.0]\n",
      "named_cosine [[0.0], [0.0], [0.0], [0.0], [0.0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                        | 5/25413 [00:00<46:10,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags ['NN']\n",
      "(2, 17)\n",
      "tfid   (0, 8)\t0.5\n",
      "  (0, 0)\t0.5\n",
      "  (0, 9)\t0.5\n",
      "  (0, 6)\t0.5\n",
      "  (1, 3)\t0.27735009811261463\n",
      "  (1, 12)\t0.27735009811261463\n",
      "  (1, 10)\t0.27735009811261463\n",
      "  (1, 2)\t0.27735009811261463\n",
      "  (1, 13)\t0.27735009811261463\n",
      "  (1, 1)\t0.27735009811261463\n",
      "  (1, 15)\t0.27735009811261463\n",
      "  (1, 16)\t0.27735009811261463\n",
      "  (1, 14)\t0.27735009811261463\n",
      "  (1, 4)\t0.27735009811261463\n",
      "  (1, 11)\t0.27735009811261463\n",
      "  (1, 5)\t0.27735009811261463\n",
      "  (1, 7)\t0.27735009811261463\n",
      "cosine [[1.0, 0.0], [0.0, 1.0000000000000007]]\n",
      "stance_cosine [0.0]\n",
      "named_cosine [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
      "tags ['NN']\n",
      "(2, 6)\n",
      "tfid   (0, 0)\t0.5773502691896257\n",
      "  (0, 2)\t0.5773502691896257\n",
      "  (0, 1)\t0.5773502691896257\n",
      "  (1, 4)\t0.5773502691896257\n",
      "  (1, 5)\t0.5773502691896257\n",
      "  (1, 3)\t0.5773502691896257\n",
      "cosine [[1.0, 0.0], [0.0, 1.0]]\n",
      "stance_cosine [0.0]\n",
      "named_cosine [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                        | 7/25413 [00:00<43:45,  9.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags ['NN']\n",
      "(2, 17)\n",
      "tfid   (0, 16)\t0.47107781233161794\n",
      "  (0, 1)\t0.33517574332792605\n",
      "  (0, 15)\t0.47107781233161794\n",
      "  (0, 3)\t0.47107781233161794\n",
      "  (0, 5)\t0.47107781233161794\n",
      "  (1, 1)\t0.20119467558491574\n",
      "  (1, 6)\t0.2827720964717407\n",
      "  (1, 7)\t0.2827720964717407\n",
      "  (1, 9)\t0.2827720964717407\n",
      "  (1, 4)\t0.2827720964717407\n",
      "  (1, 10)\t0.2827720964717407\n",
      "  (1, 0)\t0.2827720964717407\n",
      "  (1, 14)\t0.2827720964717407\n",
      "  (1, 2)\t0.2827720964717407\n",
      "  (1, 12)\t0.2827720964717407\n",
      "  (1, 8)\t0.2827720964717407\n",
      "  (1, 13)\t0.2827720964717407\n",
      "  (1, 11)\t0.2827720964717407\n",
      "cosine [[1.0, 0.06743557494279506], [0.06743557494279506, 1.0000000000000007]]\n",
      "stance_cosine [0.06743557494279506]\n",
      "named_cosine [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.06743557494279506]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                        | 8/25413 [00:00<54:30,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags ['NN']\n",
      "(2, 14)\n",
      "tfid   (0, 8)\t0.5773502691896257\n",
      "  (0, 10)\t0.5773502691896257\n",
      "  (0, 12)\t0.5773502691896257\n",
      "  (1, 9)\t0.26726124191242445\n",
      "  (1, 5)\t0.5345224838248489\n",
      "  (1, 1)\t0.26726124191242445\n",
      "  (1, 4)\t0.26726124191242445\n",
      "  (1, 6)\t0.26726124191242445\n",
      "  (1, 2)\t0.26726124191242445\n",
      "  (1, 7)\t0.26726124191242445\n",
      "  (1, 11)\t0.26726124191242445\n",
      "  (1, 13)\t0.26726124191242445\n",
      "  (1, 0)\t0.26726124191242445\n",
      "  (1, 3)\t0.26726124191242445\n",
      "cosine [[1.0, 0.0], [0.0, 1.0000000000000007]]\n",
      "stance_cosine [0.0]\n",
      "named_cosine [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.06743557494279506], [0.0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                        | 9/25413 [00:00<50:51,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags ['NN']\n",
      "(2, 12)\n",
      "tfid   (0, 11)\t0.5773502691896257\n",
      "  (0, 7)\t0.5773502691896257\n",
      "  (0, 0)\t0.5773502691896257\n",
      "  (1, 10)\t0.23570226039551584\n",
      "  (1, 1)\t0.4714045207910317\n",
      "  (1, 5)\t0.23570226039551584\n",
      "  (1, 9)\t0.23570226039551584\n",
      "  (1, 4)\t0.23570226039551584\n",
      "  (1, 3)\t0.4714045207910317\n",
      "  (1, 2)\t0.4714045207910317\n",
      "  (1, 6)\t0.23570226039551584\n",
      "  (1, 8)\t0.23570226039551584\n",
      "cosine [[1.0, 0.0], [0.0, 1.0]]\n",
      "stance_cosine [0.0]\n",
      "named_cosine [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.06743557494279506], [0.0], [0.0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "i=0\n",
    "def get_tags(text):\n",
    "    return pos_tag(word_tokenize(text))\n",
    "def filter_pos(named_tags, tag):\n",
    "    return \" \".join([stemmer.stem(name[0]) for name in named_tags if name[1].startswith(tag)])\n",
    "named_cosine = []\n",
    "tags = [\"NN\"]\n",
    "for stance in tqdm.tqdm(_stances):\n",
    "    #print(stance)\n",
    "    stance_cosine = []\n",
    "    head = get_tags(stance['originalHeadline'])\n",
    "    #print(head)\n",
    "    body = get_tags(_original_articles.get(stance['Body ID']).decode('utf-8')[:255])\n",
    "    #print(body)\n",
    "    print(\"tags\",tags)\n",
    "    for tag in tags:\n",
    "        head_f = filter_pos(head, tag)\n",
    "        body_f = filter_pos(body, tag)\n",
    "        #print(\"hf\",head_f)\n",
    "        #print(\"bf\",body_f)\n",
    "        if head_f and body_f:\n",
    "            vect = TfidfVectorizer(min_df=1)\n",
    "            tfidf = vect.fit_transform([head_f,body_f])\n",
    "            print(np.shape(tfidf))\n",
    "            print(\"tfid\",tfidf)\n",
    "            #print(\"HELLO\")\n",
    "            #print(\"INVERSE\",tfidf*tfidf.T)\n",
    "            cosine = (tfidf * tfidf.T).todense().tolist()\n",
    "            print(\"cosine\",cosine)\n",
    "            #print(len(cosine))\n",
    "            if len(cosine) == 2:\n",
    "                stance_cosine.append(cosine[1][0])\n",
    "            else:\n",
    "                stance_cosine.append(0)\n",
    "        else:\n",
    "            stance_cosine.append(0)\n",
    "        print(\"stance_cosine\",stance_cosine)\n",
    "    named_cosine.append(stance_cosine)\n",
    "    print(\"named_cosine\",named_cosine)\n",
    "    i=i+1\n",
    "    if(i==10):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
