{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, logging, re, nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import feature_extraction\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "class FeatureData(object):\n",
    "    def __init__(self, article_file_path, stances_file_path):\n",
    "        self.number_of_classes = 4\n",
    "        self.classes = ['agree', 'disagree', 'discuss', 'unrelated']\n",
    "        self.articles = self._get_articles(article_file_path)  # list of dictionaries\n",
    "        self.stances = self._get_stances(stances_file_path)\n",
    "        self.number_of_stances = len(self.stances)\n",
    "        self.number_of_articles = len(self.articles)\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_clean_articles(self):\n",
    "        clean_articles = []\n",
    "        print('Retrieving clean articles...')\n",
    "\n",
    "        for item in tqdm.tqdm(self.articles):\n",
    "            cleaned_article = clean(item['articleBody'])\n",
    "            tokens = tokenize_text(cleaned_article)\n",
    "            no_stop_word_tokens = remove_stopwords(tokens)\n",
    "            lemmatized_tokens = get_tokenized_lemmas(no_stop_word_tokens)\n",
    "            clean_articles.append({'articleBody': ' '.join(lemmatized_tokens),\n",
    "                                   'Body ID': item['Body ID']})\n",
    "        return {article['Body ID']: article['articleBody'] for article in clean_articles}\n",
    "\n",
    "    #We need the stop words for POS tagging to work propperly\n",
    "    def get_original_articles(self):\n",
    "        clean_articles = []\n",
    "        print('Retrieving original articles...')\n",
    "        for item in tqdm.tqdm(self.articles):\n",
    "            #cleaned_article = clean(item['articleBody'])\n",
    "            cleaned_article = item['articleBody'].encode('ascii', 'ignore')\n",
    "            clean_articles.append({'articleBody':cleaned_article,\n",
    "                                   'Body ID': item['Body ID']})\n",
    "        return {article['Body ID']: article['articleBody'] for article in clean_articles}\n",
    "\n",
    "    def get_clean_stances(self):\n",
    "        clean_headlines = []\n",
    "        print('Retrieving clean stances...')\n",
    "\n",
    "        for item in tqdm.tqdm(self.stances):\n",
    "            cleaned_headline = clean(item['Headline'])\n",
    "            tokens = tokenize_text(cleaned_headline)\n",
    "            no_stop_word_tokens = remove_stopwords(tokens)\n",
    "            lemmatized_tokens = get_tokenized_lemmas(no_stop_word_tokens)\n",
    "            clean_headlines.append({'Headline': ' '.join(lemmatized_tokens),\n",
    "                                    'originalHeadline': cleaned_headline,\n",
    "                                    'Body ID': item['Body ID'],\n",
    "                                    'Stance': item['Stance']})\n",
    "\n",
    "        return clean_headlines\n",
    "\n",
    "    def _get_articles(self, path):\n",
    "        # Body ID, articleBody\n",
    "        articles = []\n",
    "        dt=pd.read_csv(path)\n",
    "        articles=dt.T.to_dict().values()\n",
    "        return articles\n",
    "\n",
    "    def _get_stances(self, path):\n",
    "        # Headline, Body ID, Stance\n",
    "        stances = []\n",
    "        dt1=pd.read_csv(path)\n",
    "        stances=dt1.T.to_dict().values()\n",
    "        return stances\n",
    "\n",
    "\n",
    "def normalize_word(w):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(w).lower()\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    return \" \".join(re.findall(r'\\w+', text, flags=re.UNICODE)).lower()\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return [token for token in word_tokenize(text)]\n",
    "\n",
    "\n",
    "def remove_stopwords(list_of_tokens):\n",
    "    return [word for word in list_of_tokens if word not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
    "\n",
    "\n",
    "def get_tokenized_lemmas(tokens):\n",
    "    return [normalize_word(token) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\binni\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\binni\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk, sent_tokenize\n",
    "from gensim import models\n",
    "from gensim.models.phrases import Phraser\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.chunk import tree2conlltags\n",
    "import textacy as textacy\n",
    "from textacy.doc import Doc\n",
    "from textacy.extract import direct_quotations\n",
    "import spacy\n",
    "en=textacy.load_spacy('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureGenerator(object):\n",
    "\n",
    "\n",
    "    def __init__(self, clean_articles, clean_stances, original_articles, load_data=True):\n",
    "        self._articles = clean_articles  # dictionary {article ID: body}\n",
    "        self._original_articles = original_articles\n",
    "        self._stances = clean_stances  # list of dictionaries\n",
    "        self._max_ngram_size = 3\n",
    "        self._refuting_words = ['fake', 'fraud', 'hoax', 'false', 'deny', 'denies', 'not', 'despite', 'nope', 'doubt',\n",
    "                                'doubts', 'bogus', 'debunk', 'pranks', 'retract']\n",
    "    def get_features_from_file(features_directory, use=[]):\n",
    "        features = []\n",
    "        feature_names = []\n",
    "        for feature_csv in os.listdir(features_directory):\n",
    "            for feature in use:\n",
    "                if np.count_nonzero([feature_csv.startswith(feature)]):\n",
    "                    ##print(feature)\n",
    "                    with open(os.path.join(features_directory, feature_csv)) as f:\n",
    "                        content = np.loadtxt(fname=f, delimiter=',', skiprows=1)\n",
    "                        del_indices = []\n",
    "                        i=0\n",
    "                        if len(content.shape) == 1:\n",
    "                            content = content.reshape(content.shape[0], 1)\n",
    "                            feature_names.append(basename(feature) + str(0))\n",
    "                        else:\n",
    "                            for i in range(len(content)):\n",
    "                                if i in use[feature]:\n",
    "                                    feature_names.append(basename(feature) + str(i))\n",
    "                                else:\n",
    "                                    del_indices.append(i)\n",
    "                            content = np.delete(content, del_indices, 1)\n",
    "                       # print(feature_names)\n",
    "                        features.append(content)\n",
    "                       # print(features)\n",
    "        test = np.concatenate(features, axis=1)\n",
    "        return test, feature_names\n",
    "        \n",
    "    \n",
    "    def get_features(self, features_directory=\"features\"):\n",
    "        feature_names = []\n",
    "        features = []\n",
    "        if True:\n",
    "            print('Retrieving headline ngrams...')\n",
    "            ngrams = np.array(self._get_ngrams())\n",
    "            features.append(ngrams)\n",
    "            ngram_headings = [('ngram_' + str(count)) for count in range(1, self._max_ngram_size + 1)]\n",
    "            feature_names.append(ngram_headings)\n",
    "            self._feature_to_csv(ngrams, ngram_headings,\"C:/Users/binni/minor/\"+features_directory+'/ngrams.csv')\n",
    "\n",
    "        if True:\n",
    "            print('Retrieving word2Vec...')\n",
    "            word2Vec = np.array(self._get_word2vec()).reshape(len(self._stances), 1)\n",
    "            features.append(word2Vec)\n",
    "            feature_names.append(\"word2Vec\")\n",
    "            self._feature_to_csv(word2Vec, [\"word2Vec\"], \"C:/Users/binni/minor/\"+features_directory + '/word2Vec.csv')\n",
    "\n",
    "        if True:\n",
    "            print('Retrieving refuting words...')\n",
    "            refuting = np.array(self._get_refuting_words())\n",
    "            features.append(refuting)\n",
    "            [feature_names.append(word + '_refuting') for word in self._refuting_words]\n",
    "            self._feature_to_csv(refuting, self._refuting_words, \"C:/Users/binni/minor/\"+features_directory+'/refuting.csv')\n",
    "\n",
    "        if True:\n",
    "            print('Retrieving polarity...')\n",
    "            polarity = np.array(self._polarity_feature())\n",
    "            features.append(polarity)\n",
    "            feature_names.append('headline_polarity')\n",
    "            feature_names.append('article_polarity')\n",
    "            self._feature_to_csv(polarity, ['headline_polarity', 'article_polarity'],\"C:/Users/binni/minor/\"+ features_directory+'/polarity.csv')\n",
    "        if True:\n",
    "            print('Retrieving named entity cosine...')\n",
    "            named_cosine = np.array(self._named_entity_feature()).reshape(len(self._stances), 1)\n",
    "            features.append(named_cosine)\n",
    "            feature_names.append('named_cosine')\n",
    "            self._feature_to_csv(named_cosine, ['named_cosine'],\"C:/Users/binni/minor/\"+ features_directory+'/named_cosine.csv')\n",
    "\n",
    "        if True:\n",
    "            print('Retrieving VADER...')\n",
    "            vader = np.array(self._vader_feature()).reshape(len(self._stances), 2)\n",
    "            features.append(vader)\n",
    "            feature_names.append('vader_pos')\n",
    "            feature_names.append('vader_neg')\n",
    "            self._feature_to_csv(vader, ['vader'],\"C:/Users/binni/minor/\"+ features_directory+'/vader.csv')\n",
    "\n",
    "        if True:\n",
    "            print('Retrieving jaccard similarities...')\n",
    "            jaccard = np.array(self._get_jaccard_similarity()).reshape(len(self._stances), 1)\n",
    "            features.append(jaccard)\n",
    "            feature_names.append('jaccard_similarity')\n",
    "            self._feature_to_csv(jaccard, ['jaccard_similarity'],\"C:/Users/binni/minor/\"+ features_directory+'/jaccard_similarity.csv')\n",
    "\n",
    "        if True:\n",
    "            print('Retrieving quote analysis...')\n",
    "            quotes = np.array(self._get_quotes()).reshape(len(self._stances), 1)\n",
    "            features.append(quotes)\n",
    "            feature_names.append('quote_analysis')\n",
    "            self._feature_to_csv(quotes, ['quote_analysis'],\"C:/Users/binni/minor/\"+features_directory+'/quote_analysis.csv')\n",
    "\n",
    "        if True:\n",
    "            lengths = np.array(self._length_feature()).reshape(len(self._stances), 1)\n",
    "            features.append(lengths)\n",
    "            feature_names.append('lengths')\n",
    "            self._feature_to_csv(lengths, ['lengths'], \"C:/Users/binni/minor/\"+features_directory + '/lengths.csv')\n",
    "\n",
    "    def _feature_to_csv(self, feature, feature_headers, output_path):\n",
    "        header = ','.join(feature_headers)\n",
    "        np.savetxt(fname=output_path, X=feature, delimiter=',', header=header, comments='')\n",
    "\n",
    "    def _get_ngrams(self):\n",
    "        ngrams = []\n",
    "\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            # Retrieves the vocabulary of ngrams for the headline.\n",
    "            stance_vectorizer = CountVectorizer(input=stance['Headline'], ngram_range=(1, self._max_ngram_size),\n",
    "                                                binary=True)\n",
    "            stance_vectorizer.fit_transform([stance['Headline']]).toarray()\n",
    "\n",
    "            # Search the article text and count headline ngrams.\n",
    "            vocab = stance_vectorizer.get_feature_names()\n",
    "            vectorizer = CountVectorizer(input=self._articles[stance['Body ID']], vocabulary=vocab,\n",
    "                                         ngram_range=(1, self._max_ngram_size))\n",
    "            ngram_counts = vectorizer.fit_transform([self._articles[stance['Body ID']]]).toarray()\n",
    "            features = vectorizer.get_feature_names()\n",
    "\n",
    "            aggregated_counts = [0 for _ in range(self._max_ngram_size)]\n",
    "\n",
    "            # Create a list of the aggregated counts of each ngram size.\n",
    "            for index in np.nditer(np.nonzero(ngram_counts[0]), ['zerosize_ok']):\n",
    "                aggregated_counts[len(features[index].split()) - 1] += ngram_counts[0][index]\n",
    "\n",
    "            # attempt to standardize ngram counts across headlines and bodies of varying length by dividing total\n",
    "            # ngram hits by the length of the headline. These will need to be normalized later so they lie\n",
    "            # between 0 and 1.\n",
    "            standardized_counts = [1.0*count/len(stance['Headline'].split()) for count in aggregated_counts]\n",
    "\n",
    "            ngrams.append(standardized_counts)\n",
    "            #print ngrams\n",
    "\n",
    "        return ngrams\n",
    "\n",
    "    def _get_word2vec(self):\n",
    "        # Gather sentences\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        all_words = []; atricle_words = []\n",
    "\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            if stance['Stance'] == 'unrelated':\n",
    "                pass\n",
    "            body_words = []; headline_words = []\n",
    "            headline = tokenizer.tokenize(stance['originalHeadline'])\n",
    "            body = (tokenizer.tokenize(self._original_articles[stance['Body ID']].decode('utf-8')))[:4]\n",
    "            for s in headline:\n",
    "                s = word_tokenize(s)\n",
    "                headline_words = headline_words + s\n",
    "                all_words.append(s)\n",
    "            for s in body:\n",
    "                s = word_tokenize(s)\n",
    "                body_words = body_words + s\n",
    "                all_words.append(s)\n",
    "            atricle_words.append([headline_words, body_words])\n",
    "\n",
    "        # Train Word2Vec\n",
    "        model = models.Word2Vec(all_words, size=100, min_count=1)\n",
    "\n",
    "        cosine_similarities = []\n",
    "        # Generate sentence vectors and computer cosine similarity\n",
    "        for headline, body in atricle_words:\n",
    "            h_vector = sum([model.wv[word] for word in headline])\n",
    "            b_vector = sum([model.wv[word] for word in body])\n",
    "            cosine_similarities.append(cosine_similarity(h_vector.reshape(1,-1), b_vector.reshape(1,-1)))\n",
    "\n",
    "        return cosine_similarities\n",
    "\n",
    "    def _get_refuting_words(self):\n",
    "        \"\"\" Retrieves headlines of the articles and indicates a count of each of the refuting words in the headline.\n",
    "        Returns a list containing the number of refuting words found (at lease once) in the headline. \"\"\"\n",
    "\n",
    "        features = []\n",
    "\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            # print \"[DEBUG] stance \", stance\n",
    "            count = [1 if refute_word in stance['Headline'] else 0 for refute_word in self._refuting_words]\n",
    "            # print \"[DEBUG] count \", count\n",
    "            features.append(count)\n",
    "        # print \"[DEBUG] features\", features\n",
    "        return features\n",
    "\n",
    "    def _polarity_feature(self):\n",
    "        _refuting_words = ['fake', 'fraud', 'hoax', 'false', 'deny', 'denies', 'not',\n",
    "                           'despite', 'nope', 'nowhere', 'doubt', 'doubts', 'bogus', 'debunk', 'pranks',\n",
    "                           'retract', 'nothing', 'never', 'none', 'budge']\n",
    "\n",
    "        def determine_polarity(text):\n",
    "            tokens = tokenize_text(text)\n",
    "            return sum([token in _refuting_words for token in tokens]) % 2\n",
    "\n",
    "        polarities = []\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            headline_polarity = determine_polarity(stance['Headline'])\n",
    "            body_polarity = determine_polarity(self._articles.get(stance['Body ID']))\n",
    "            polarities.append([headline_polarity, body_polarity])\n",
    "\n",
    "        return polarities\n",
    "\n",
    "    def _named_entity_feature(self):\n",
    "        \"\"\" Retrieves a list of Named Entities from the Headline and Body.\n",
    "        Returns a list containing the cosine similarity between the counts of the named entities \"\"\"\n",
    "        stemmer = PorterStemmer()\n",
    "        def get_tags(text):\n",
    "            return pos_tag(word_tokenize(text))\n",
    "\n",
    "        def filter_pos(named_tags, tag):\n",
    "            return \" \".join([stemmer.stem(name[0]) for name in named_tags if name[1].startswith(tag)])\n",
    "\n",
    "        named_cosine = []\n",
    "        tags = [\"NN\"]\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            stance_cosine = []\n",
    "            head = get_tags(stance['originalHeadline'])\n",
    "            body = get_tags(self._original_articles.get(stance['Body ID']).decode('utf-8')[:255])\n",
    "\n",
    "            for tag in tags:\n",
    "                head_f = filter_pos(head, tag)\n",
    "                body_f = filter_pos(body, tag)\n",
    "\n",
    "                if head_f and body_f:\n",
    "                    vect = TfidfVectorizer(min_df=1)\n",
    "                    tfidf = vect.fit_transform([head_f,body_f])\n",
    "                    cosine = (tfidf * tfidf.T).todense().tolist()\n",
    "                    if len(cosine) == 2:\n",
    "                        stance_cosine.append(cosine[1][0])\n",
    "                    else:\n",
    "                        stance_cosine.append(0)\n",
    "                else:\n",
    "                    stance_cosine.append(0)\n",
    "            named_cosine.append(stance_cosine)\n",
    "        return named_cosine\n",
    "\n",
    "    def _vader_feature(self):\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        features = []\n",
    "\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            headVader = sid.polarity_scores(stance[\"Headline\"])\n",
    "            bodyVader = sid.polarity_scores(sent_tokenize(self._original_articles.get(stance['Body ID']).decode('utf-8'))[0])\n",
    "            features.append(abs(headVader['pos']-bodyVader['pos']))\n",
    "            features.append(abs(headVader['neg']-bodyVader['neg']))\n",
    "        return features\n",
    "\n",
    "    def _get_jaccard_similarity(self):\n",
    "        \"\"\" Get the jaccard similarities for each headline and article body pair. Jaccard similarity is defined as\n",
    "        J(A, B) = |A intersect B| / |A union B|. Try to normalize by only considering the first\"\"\"\n",
    "        similarities = []\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            headline = set(stance['Headline'].split())\n",
    "            body = set(self._articles.get(stance['Body ID']).split()[:255])\n",
    "            jaccard = float(len(headline.intersection(body))) / len(headline.union(body))\n",
    "            similarities.append(jaccard)\n",
    "\n",
    "        return similarities\n",
    "\n",
    "    def _get_quotes(self):\n",
    "        quote_count = []\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            body = self._original_articles.get(stance['Body ID']).decode('utf-8', 'replace')\n",
    "            doc = Doc(content=body,lang=en)\n",
    "            quotes = direct_quotations(doc)\n",
    "            quote_counter = 0\n",
    "\n",
    "            for q in quotes:\n",
    "                quote_counter = quote_counter + len(q[2])\n",
    "            quote_counter = quote_counter / len(body)\n",
    "            quote_count.append(quote_counter)\n",
    "\n",
    "        return quote_count\n",
    "\n",
    "    def _length_feature(self):\n",
    "        lengths = []\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            lengths.append(len(self._original_articles.get(stance['Body ID'])))\n",
    "        return lengths\n",
    "\n",
    "    def _get_punctuation_frequency(self):\n",
    "        frequencies = []\n",
    "\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            question_marks = 0\n",
    "            exclamation_marks = 0\n",
    "            article_body = self._original_articles[stance['Body ID']]\n",
    "\n",
    "            for character in article_body:\n",
    "                if character == '?':\n",
    "                    question_marks += 1\n",
    "                elif character == '!':\n",
    "                    exclamation_marks += 1\n",
    "\n",
    "            frequency = (question_marks + exclamation_marks) / len(article_body.split())\n",
    "            frequencies.append(frequency)\n",
    "\n",
    "        return frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving clean articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1683/1683 [00:50<00:00, 33.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving clean stances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███████████▊                       | 16929/49972 [00:31<00:59, 556.17it/s]"
     ]
    }
   ],
   "source": [
    "feature_data = FeatureData('C:\\\\Users\\\\binni\\\\minor\\\\data\\\\train_bodies.csv', 'C:\\\\Users\\\\binni\\\\minor\\\\data\\\\train_stances.csv')\n",
    "feature_generator = FeatureGenerator(feature_data.get_clean_articles(), feature_data.get_clean_stances(), feature_data.get_original_articles())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = feature_generator.get_features(\"feature_generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, modelType, features):\n",
    "        self._stance_map = {'unrelated': 0, 'discuss': 1, 'agree': 2, 'disagree': 3}\n",
    "        self._model_type = modelType\n",
    "        self._features_for_X1 = features\n",
    "        self._feature_col_names = []\n",
    "\n",
    "    def get_data(self, body_file, stance_file, features_directory):\n",
    "        feature_data = FeatureData(body_file, stance_file)\n",
    "        X_train, self._feature_col_names = FeatureGenerator.get_features_from_file(use=self._features_for_X1,\n",
    "                                                        features_directory=features_directory)\n",
    "\n",
    "        y_train = np.asarray([self._stance_map[stance['Stance']] for stance in feature_data.stances])\n",
    "\n",
    "        \n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        X_train = min_max_scaler.fit_transform(X_train)\n",
    "\n",
    "        return {'X':X_train, 'y':y_train}\n",
    "    def get_trained_classifier(self, X_train, y_train):\n",
    "        \n",
    "        if self._model_type == 'svm':\n",
    "            classifier = svm.SVC(decision_function_shape='ovr', cache_size=1000)\n",
    "        elif self._model_type == 'nn':\n",
    "            classifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(30,), random_state=1)\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        return classifier\n",
    "    def test_classifier(self, classifier, X_test):\n",
    "        return classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_features = {\n",
    "    #'refuting': [0,2,3,8,12,13],\n",
    "    'ngrams': [0, 1, 2],\n",
    "    #'polarity': [0],\n",
    "    'named': [],\n",
    "    #'vader': [0,1],\n",
    "    'jaccard': [],\n",
    "    'quote_analysis': [],\n",
    "    'lengths': [],\n",
    "    'punctuation_frequency': [],\n",
    "    'word2Vec': []\n",
    "}\n",
    "X2_features = {\n",
    "    #'refuting': [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14],\n",
    "    'ngrams': [1],\n",
    "    'polarity': [1],\n",
    "    #'named': [],\n",
    "    #'vader': [0,1],\n",
    "    #'jaccard': [],\n",
    "    'quote_analysis': [],\n",
    "    'lengths': [],\n",
    "    'punctuation_frequency': [],\n",
    "    #'word2Vec': []\n",
    "}\n",
    "\n",
    "model1_type = 'nn'\n",
    "model2_type = 'nn'\n",
    "doStratify = False\n",
    "doKfold = False\n",
    "numFolds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify(X, y):\n",
    "\n",
    "  \n",
    "    disagree_indices = np.where(y == 3)[0]\n",
    "    agree_indices = np.where(y == 2)[0]\n",
    "    discuss_indices = np.where(y == 1)[0]\n",
    "    unrelated_indices = np.where(y == 0)[0]\n",
    "\n",
    "    num_disagree = disagree_indices.shape[0]\n",
    "\n",
    "   \n",
    "    reduced_agree_indices = agree_indices[:len(agree_indices)]\n",
    "    reduced_discuss_indices = discuss_indices[:len(discuss_indices)]\n",
    "    reduced_unrelated_indices = unrelated_indices[:(num_disagree + len(agree_indices) + len(discuss_indices))]\n",
    "    X_stratified = np.concatenate([X[disagree_indices], X[reduced_agree_indices], X[reduced_discuss_indices],\n",
    "                                   X[reduced_unrelated_indices]], axis=0)\n",
    "    y_stratified = np.concatenate([y[disagree_indices], y[reduced_agree_indices], y[reduced_discuss_indices],\n",
    "                                   y[reduced_unrelated_indices]], axis=0)\n",
    "\n",
    "    return {'X': X_stratified, 'y': y_stratified}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_stances(y):\n",
    "    stance_map = {0: 'unrelated', 1: 'discuss', 2: 'agree', 3: 'disagree'}\n",
    "    return [stance_map.get(key) for key in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename\n",
    "from sklearn import svm, preprocessing\n",
    "import os, re, string, tqdm, nltk\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model(model1_type, X1_features)\n",
    "model2 = Model(model2_type, X2_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\binni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train1 = model1.get_data('C:/Users/binni/minor/data/train_bodies.csv', 'C:/Users/binni/minor/data/train_stances.csv', 'features')\n",
    "test1  = model1.get_data('C:/Users/binni/minor/data/competition_test_bodies.csv', 'C:/Users/binni/minor/data/competition_test_stances.csv', 'test_features')\n",
    "train2 = model2.get_data('C:/Users/binni/minor/data/train_bodies.csv', 'C:/Users/binni/minor/data/train_stances.csv', 'features')\n",
    "test2  = model2.get_data('C:/Users/binni/minor/data/competition_test_bodies.csv', 'C:/Users/binni/minor/data/competition_test_stances.csv', 'test_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data1, data2, doStratify):\n",
    "    X1 = data1['X']; X2 = data2['X']\n",
    "    y1 = data1['y']; y2 = data2['y']\n",
    "\n",
    "    if doStratify:\n",
    "        stratified = stratify(X1, y1)\n",
    "        X1 = stratified['X']\n",
    "        y1 = stratified['y']\n",
    "        X2 = stratified['X']\n",
    "        y2 = stratified['y']\n",
    "\n",
    "    return X1, y1, X2, y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, y1_train, X1_test, y1_test = split_data(train1, test1, doStratify)\n",
    "X2_train, y2_train, X2_test, y_test = split_data(train2, test2, doStratify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_train1 = [int(s != 0) for s in y1_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train_filtered = X2_train[np.nonzero(y1_train1)]\n",
    "y2_train_filtered = y2_train[np.nonzero(y1_train1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12240453, 0.00574713, 1.        , 0.        , 0.68661899],\n",
       "       [0.08280093, 0.03448276, 0.        , 0.        , 0.        ],\n",
       "       [0.09833745, 0.02955665, 0.        , 0.        , 0.        ],\n",
       "       ...,\n",
       "       [0.10022506, 0.06896552, 0.        , 0.06620763, 0.32223034],\n",
       "       [0.12476405, 0.01253918, 0.        , 0.        , 0.05381519],\n",
       "       [0.02334108, 0.03448276, 0.        , 0.27901786, 0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = model1.get_trained_classifier(X1_train, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = model2.get_trained_classifier(X2_train_filtered, y2_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted  = model1.test_classifier(clf1, X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_predicted = model2.test_classifier(clf2, X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_test = map_stances([int(s != 0) for s in y_test])\n",
    "tmp_predicted = map_stances(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
    "LABELS_RELATED = ['unrelated','related']\n",
    "RELATED = LABELS[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(cm):\n",
    "    lines = []\n",
    "    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n",
    "    line_len = len(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "    lines.append(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    for i, row in enumerate(cm):\n",
    "        hit += row[i]\n",
    "        total += sum(row)\n",
    "        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n",
    "                                                                   *row))\n",
    "        lines.append(\"-\"*line_len)\n",
    "    print('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_submission(gold_labels, test_labels):\n",
    "    score = 0.0\n",
    "    cm = [[0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0]]\n",
    "\n",
    "    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n",
    "       # print(i,g,t)\n",
    "        g_stance, t_stance = g, t\n",
    "        if g_stance == t_stance:\n",
    "            score += 0.25\n",
    "            if g_stance != 'unrelated':\n",
    "                score += 0.50\n",
    "        if g_stance in RELATED and t_stance in RELATED:\n",
    "            score += 0.25\n",
    "\n",
    "        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n",
    "\n",
    "    return score, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_score(actual, predicted):\n",
    "    score, cm = score_submission(actual, predicted)\n",
    "    ##print(score,cm)\n",
    "    best_score, _ = score_submission(actual, actual)\n",
    "\n",
    "    print_confusion_matrix(cm)\n",
    "    print(\"Score: \" +str(score) + \" out of \" + str(best_score) + \"\\t(\"+str(score*100/best_score) + \"%)\")\n",
    "    return score*100/best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(actual, predicted, stance_map):\n",
    "    pairs = zip(actual, predicted)\n",
    "    print(\"Precision\")\n",
    "    scores = {stance: None for stance in stance_map.items()}\n",
    "    for stance, index in stance_map.items():\n",
    "        truePositive = np.count_nonzero([x[1] == index for x in pairs if x[0] == index])\n",
    "        falsePositive = np.count_nonzero([x[1] == index for x in pairs if x[0] != index])\n",
    "        try:\n",
    "            precision = 100 * float(truePositive) / (truePositive + falsePositive + 1)\n",
    "            scores[stance] = precision\n",
    "            #print(stance + \": \" + str(precision))\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "           # print(\"Zero\")\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(actual, predicted, stance_map):\n",
    "    print(\"Recall\")\n",
    "    pairs = zip(actual, predicted)\n",
    "    scores = {stance: None for stance in stance_map.items()}\n",
    "    for stance, index in stance_map.items():\n",
    "        truePositive = np.count_nonzero([x[1] == index for x in pairs if x[0] == index])\n",
    "        falseNegative = np.count_nonzero([x[1] != index for x in pairs if x[0] == index])\n",
    "        try:\n",
    "            recall = 100 * float(truePositive) / (truePositive + falseNegative + 1)\n",
    "            scores[stance] = recall\n",
    "            #print(stance + \": \" + str(recall))\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "            #print(\"Zero\")\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(actual, predicted, stance_map):\n",
    "    print(\"Accuracy\")\n",
    "    pairs = zip(actual, predicted)\n",
    "    scores = {stance: None for stance in stance_map.items()}\n",
    "    for stance, index in stance_map.items():\n",
    "        accurate = np.count_nonzero([x[1] == index and x[1] == x[0] for x in pairs])\n",
    "        total = np.count_nonzero([x[0] == index for x in pairs])\n",
    "        try:\n",
    "            accuracy = 100 * float(accurate)/total\n",
    "            scores[stance] = accuracy\n",
    "            #print(stance + \": \" + str(accuracy))\n",
    "        except ZeroDivisionError:\n",
    "           # print(\"Zero\"\n",
    "            pass\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |     0     |     0     |     0     |     0     |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |     0     |     0     |     0     |     0     |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    333    |     5     |   12562   |    527    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     0     |     0     |    326    |   13101   |\n",
      "-------------------------------------------------------------\n",
      "Score: 15921.75 out of 16783.75\t(94.86407983913011%)\n"
     ]
    }
   ],
   "source": [
    "tmp_competition_score = report_score(tmp_test, tmp_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 1\n",
      "2 1\n",
      "3 1\n",
      "4 1\n",
      "5 1\n",
      "6 1\n",
      "7 1\n",
      "8 1\n",
      "9 1\n",
      "10 1\n",
      "11 1\n",
      "12 1\n",
      "13 1\n",
      "14 1\n",
      "15 1\n",
      "16 1\n",
      "17 1\n",
      "18 1\n",
      "19 1\n",
      "20 1\n",
      "21 1\n",
      "22 1\n",
      "23 1\n",
      "24 1\n",
      "25 1\n",
      "26 1\n",
      "27 1\n",
      "28 0\n",
      "29 1\n",
      "30 1\n",
      "31 1\n",
      "32 1\n",
      "33 1\n",
      "34 1\n",
      "35 1\n",
      "36 1\n",
      "37 1\n",
      "38 1\n",
      "39 1\n",
      "40 1\n",
      "41 1\n",
      "42 1\n",
      "43 1\n",
      "44 1\n",
      "45 1\n",
      "46 1\n",
      "47 1\n",
      "48 1\n",
      "49 1\n",
      "50 1\n",
      "51 1\n",
      "52 1\n",
      "53 1\n",
      "54 1\n",
      "55 1\n",
      "56 1\n",
      "57 1\n",
      "58 0\n",
      "59 0\n",
      "60 1\n",
      "61 1\n",
      "62 1\n",
      "63 1\n",
      "64 1\n",
      "65 1\n",
      "66 1\n",
      "67 1\n",
      "68 1\n",
      "69 1\n",
      "70 0\n",
      "71 1\n",
      "72 1\n",
      "73 1\n",
      "74 1\n",
      "75 1\n",
      "76 1\n",
      "77 0\n",
      "78 1\n",
      "79 1\n",
      "80 1\n",
      "81 1\n",
      "82 1\n",
      "83 1\n",
      "84 1\n",
      "85 1\n",
      "86 1\n",
      "87 1\n",
      "88 1\n",
      "89 1\n",
      "90 1\n",
      "91 1\n",
      "92 1\n",
      "93 1\n",
      "94 1\n",
      "95 1\n",
      "96 1\n",
      "97 1\n",
      "98 0\n",
      "99 1\n",
      "100 1\n",
      "101 0\n",
      "102 1\n",
      "103 1\n",
      "104 1\n",
      "105 1\n",
      "106 1\n",
      "107 1\n",
      "108 1\n",
      "109 1\n",
      "110 1\n",
      "111 1\n",
      "112 1\n",
      "113 1\n",
      "114 1\n",
      "115 1\n",
      "116 1\n",
      "117 1\n",
      "118 1\n",
      "119 1\n",
      "120 1\n",
      "121 1\n",
      "122 1\n",
      "123 1\n",
      "124 1\n",
      "125 1\n",
      "126 1\n",
      "127 1\n",
      "128 1\n",
      "129 1\n",
      "130 1\n",
      "131 1\n",
      "132 1\n",
      "133 1\n",
      "134 1\n",
      "135 1\n",
      "136 1\n",
      "137 1\n",
      "138 1\n",
      "139 1\n",
      "140 1\n",
      "141 1\n",
      "142 1\n",
      "143 1\n",
      "144 1\n",
      "145 1\n",
      "146 1\n",
      "147 1\n",
      "148 1\n",
      "149 1\n",
      "150 1\n",
      "151 1\n",
      "152 1\n",
      "153 1\n",
      "154 1\n",
      "155 1\n",
      "156 1\n",
      "157 1\n",
      "158 1\n",
      "159 1\n",
      "160 1\n",
      "161 1\n",
      "162 1\n",
      "163 1\n",
      "164 1\n",
      "165 1\n",
      "166 1\n",
      "167 1\n",
      "168 1\n",
      "169 1\n",
      "170 1\n",
      "171 1\n",
      "172 1\n",
      "173 1\n",
      "174 1\n",
      "175 1\n",
      "176 1\n",
      "177 1\n",
      "178 1\n",
      "179 1\n",
      "180 0\n",
      "181 1\n",
      "182 1\n",
      "183 0\n",
      "184 1\n",
      "185 1\n",
      "186 1\n",
      "187 1\n",
      "188 1\n",
      "189 1\n",
      "190 1\n",
      "191 1\n",
      "192 1\n",
      "193 1\n",
      "194 1\n",
      "195 1\n",
      "196 1\n",
      "197 1\n",
      "198 1\n",
      "199 0\n",
      "200 1\n",
      "201 1\n",
      "202 1\n",
      "203 1\n",
      "204 1\n",
      "205 1\n",
      "206 1\n",
      "207 0\n",
      "208 1\n",
      "209 0\n",
      "210 0\n",
      "211 1\n",
      "212 1\n",
      "213 1\n",
      "214 1\n",
      "215 1\n",
      "216 1\n",
      "217 1\n",
      "218 1\n",
      "219 1\n",
      "220 1\n",
      "221 1\n",
      "222 1\n",
      "223 1\n",
      "224 1\n",
      "225 1\n",
      "226 1\n",
      "227 1\n",
      "228 0\n",
      "229 1\n",
      "230 1\n",
      "231 1\n",
      "232 1\n",
      "233 1\n",
      "234 1\n",
      "235 1\n",
      "236 1\n",
      "237 1\n",
      "238 1\n",
      "239 1\n",
      "240 1\n",
      "241 1\n",
      "242 1\n",
      "243 1\n",
      "244 1\n",
      "245 1\n",
      "246 1\n",
      "247 1\n",
      "248 1\n",
      "249 1\n",
      "250 1\n",
      "251 1\n",
      "252 1\n",
      "253 1\n",
      "254 1\n",
      "255 1\n",
      "256 1\n",
      "257 1\n",
      "258 1\n",
      "259 1\n",
      "260 1\n",
      "261 1\n",
      "262 1\n",
      "263 1\n",
      "264 1\n",
      "265 1\n",
      "266 1\n",
      "267 1\n",
      "268 1\n",
      "269 1\n",
      "270 1\n",
      "271 1\n",
      "272 1\n",
      "273 1\n",
      "274 1\n",
      "275 1\n",
      "276 1\n",
      "277 1\n",
      "278 1\n",
      "279 1\n",
      "280 1\n",
      "281 1\n",
      "282 1\n",
      "283 0\n",
      "284 1\n",
      "285 1\n",
      "286 1\n",
      "287 1\n",
      "288 1\n",
      "289 1\n",
      "290 1\n",
      "291 1\n",
      "292 1\n",
      "293 1\n",
      "294 1\n",
      "295 1\n",
      "296 1\n",
      "297 1\n",
      "298 1\n",
      "299 1\n",
      "300 1\n",
      "301 1\n",
      "302 1\n",
      "303 1\n",
      "304 1\n",
      "305 1\n",
      "306 0\n",
      "307 1\n",
      "308 1\n",
      "309 1\n",
      "310 1\n",
      "311 1\n",
      "312 1\n",
      "313 1\n",
      "314 1\n",
      "315 1\n",
      "316 1\n",
      "317 1\n",
      "318 1\n",
      "319 1\n",
      "320 1\n",
      "321 1\n",
      "322 1\n",
      "323 1\n",
      "324 1\n",
      "325 1\n",
      "326 1\n",
      "327 1\n",
      "328 1\n",
      "329 0\n",
      "330 1\n",
      "331 1\n",
      "332 1\n",
      "333 1\n",
      "334 1\n",
      "335 1\n",
      "336 1\n",
      "337 1\n",
      "338 1\n",
      "339 1\n",
      "340 1\n",
      "341 1\n",
      "342 1\n",
      "343 1\n",
      "344 1\n",
      "345 1\n",
      "346 1\n",
      "347 1\n",
      "348 1\n",
      "349 1\n",
      "350 1\n",
      "351 1\n",
      "352 1\n",
      "353 1\n",
      "354 1\n",
      "355 1\n",
      "356 1\n",
      "357 1\n",
      "358 1\n",
      "359 1\n",
      "360 1\n",
      "361 1\n",
      "362 1\n",
      "363 1\n",
      "364 1\n",
      "365 1\n",
      "366 1\n",
      "367 0\n",
      "368 1\n",
      "369 1\n",
      "370 1\n",
      "371 1\n",
      "372 1\n",
      "373 1\n",
      "374 1\n",
      "375 1\n",
      "376 1\n",
      "377 1\n",
      "378 1\n",
      "379 1\n",
      "380 1\n",
      "381 1\n",
      "382 1\n",
      "383 1\n",
      "384 1\n",
      "385 1\n",
      "386 1\n",
      "387 1\n",
      "388 1\n",
      "389 1\n",
      "390 1\n",
      "391 1\n",
      "392 1\n",
      "393 1\n",
      "394 1\n",
      "395 1\n",
      "396 1\n",
      "397 1\n",
      "398 1\n",
      "399 1\n",
      "400 1\n",
      "401 1\n",
      "402 1\n",
      "403 1\n",
      "404 1\n",
      "405 1\n",
      "406 1\n",
      "407 1\n",
      "408 1\n",
      "409 1\n",
      "410 0\n",
      "411 1\n",
      "412 1\n",
      "413 1\n",
      "414 1\n",
      "415 1\n",
      "416 1\n",
      "417 1\n",
      "418 1\n",
      "419 1\n",
      "420 1\n",
      "421 1\n",
      "422 1\n",
      "423 1\n",
      "424 1\n",
      "425 1\n",
      "426 1\n",
      "427 1\n",
      "428 1\n",
      "429 1\n",
      "430 1\n",
      "431 1\n",
      "432 1\n",
      "433 1\n",
      "434 1\n",
      "435 1\n",
      "436 1\n",
      "437 1\n",
      "438 1\n",
      "439 1\n",
      "440 1\n",
      "441 1\n",
      "442 1\n",
      "443 1\n",
      "444 1\n",
      "445 1\n",
      "446 1\n",
      "447 1\n",
      "448 1\n",
      "449 1\n",
      "450 1\n",
      "451 1\n",
      "452 1\n",
      "453 1\n",
      "454 1\n",
      "455 1\n",
      "456 1\n",
      "457 1\n",
      "458 1\n",
      "459 1\n",
      "460 1\n",
      "461 0\n",
      "462 1\n",
      "463 1\n",
      "464 1\n",
      "465 1\n",
      "466 0\n",
      "467 1\n",
      "468 1\n",
      "469 1\n",
      "470 1\n",
      "471 1\n",
      "472 1\n",
      "473 1\n",
      "474 1\n",
      "475 1\n",
      "476 1\n",
      "477 1\n",
      "478 1\n",
      "479 1\n",
      "480 1\n",
      "481 1\n",
      "482 1\n",
      "483 1\n",
      "484 1\n",
      "485 1\n",
      "486 1\n",
      "487 1\n",
      "488 1\n",
      "489 1\n",
      "490 1\n",
      "491 1\n",
      "492 1\n",
      "493 1\n",
      "494 1\n",
      "495 1\n",
      "496 1\n",
      "497 1\n",
      "498 1\n",
      "499 0\n",
      "500 1\n",
      "501 1\n",
      "502 1\n",
      "503 0\n",
      "504 1\n",
      "505 1\n",
      "506 1\n",
      "507 1\n",
      "508 1\n",
      "509 1\n",
      "510 1\n",
      "511 1\n",
      "512 1\n",
      "513 0\n",
      "514 1\n",
      "515 1\n",
      "516 1\n",
      "517 1\n",
      "518 0\n",
      "519 1\n",
      "520 1\n",
      "521 1\n",
      "522 1\n",
      "523 1\n",
      "524 1\n",
      "525 1\n",
      "526 1\n",
      "527 1\n",
      "528 1\n",
      "529 1\n",
      "530 1\n",
      "531 1\n",
      "532 0\n",
      "533 1\n",
      "534 1\n",
      "535 1\n",
      "536 1\n",
      "537 1\n",
      "538 0\n",
      "539 1\n",
      "540 0\n",
      "541 1\n",
      "542 1\n",
      "543 1\n",
      "544 1\n",
      "545 1\n",
      "546 1\n",
      "547 1\n",
      "548 1\n",
      "549 1\n",
      "550"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-d820da0e0193>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_predicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__convertor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_and_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite_and_convert\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_ansi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_plain_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite_plain_text\u001b[1;34m(self, text, start, end)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, stance in enumerate(y_predicted):\n",
    "    print(i,stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2_predicted[73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision\n",
      "Recall\n",
      "Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('agree', 2): None,\n",
       " ('disagree', 3): None,\n",
       " ('discuss', 1): None,\n",
       " ('unrelated', 0): None}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, stance in enumerate(y_predicted):\n",
    "    if stance != 0:\n",
    "        y_predicted[i] = y2_predicted[i]\n",
    "precision(y_test, y_predicted, model1._stance_map)\n",
    "recall(y_test, y_predicted, model1._stance_map)\n",
    "accuracy(y_test, y_predicted, model1._stance_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coefficients(classifier, feature_names, i, k):\n",
    "    top_features=len(feature_names)/2\n",
    "    coef = classifier.coefs_[0]\n",
    "\n",
    "    top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    "    top_negative_coefficients = np.argsort(coef)[:top_features]\n",
    "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "\n",
    "   \n",
    "    plt.figure(figsize=(30, 20))\n",
    "    colors = ['#cccccc' if c < 0 else 'teal' for c in coef[top_coefficients]]\n",
    "    plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(np.arange(0, 1 + 2 * top_features), feature_names[top_coefficients], rotation='70')\n",
    "    plt.savefig(\"graphs/plot-NN_model\" + str(i) + \"_kfold\" + str(k) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_system(X1_features, X2_features, doStratify, numFolds, m1_type, m2_type):\n",
    "    # init models\n",
    "    model1 = Model(m1_type, X1_features)\n",
    "    model2 = Model(m2_type, X2_features)\n",
    "\n",
    "    # Get training and testing data\n",
    "    data = model1.get_data('C:/Users/binni/minor/data/combined_bodies.csv', 'C:/Users/binni/minor/data/combined_stances.csv', 'combined_features')\n",
    "    data2 = model2.get_data('C:/Users/binni/minor/data/combined_bodies.csv', 'C:/Users/binni/minor/data/combined_stances.csv', 'combined_features')\n",
    "\n",
    "    X1, y1, X2, y2 = split_data(data, data2, doStratify)\n",
    "\n",
    "    # For loop parameters\n",
    "    kfold = StratifiedKFold(n_splits=numFolds)\n",
    "    precision_scores = []; recall_scores = []; \n",
    "    accuracy_scores = []; competition_scores = []\n",
    "    k=0\n",
    "\n",
    "    for train_indices, test_indices in kfold.split(X1, y1):\n",
    "        X1_train = X1[train_indices]\n",
    "        y1_train = [int(s != 0) for s in y1[train_indices]]\n",
    "        X2_train = X2[train_indices]\n",
    "        y2_train = y2[train_indices]\n",
    "\n",
    "        \n",
    "        X1_test = X1[test_indices]\n",
    "        X2_test = X2[test_indices]\n",
    "        y_test  = y2[test_indices]\n",
    "\n",
    "        \n",
    "        X2_train_filtered = X2_train[np.nonzero(y1_train)]\n",
    "        y2_train_filtered = y2_train[np.nonzero(y1_train)]\n",
    "\n",
    "        \n",
    "        clf1 = model1.get_trained_classifier(X1_train, y1_train)\n",
    "\n",
    "        \n",
    "        clf2 = model2.get_trained_classifier(X2_train_filtered, y2_train_filtered)\n",
    "       \n",
    "        y_predicted = model1.test_classifier(clf1, X1_test)\n",
    "       \n",
    "\n",
    "        y2_predicted = model2.test_classifier(clf2, X2_test)\n",
    "        \n",
    "\n",
    "       \n",
    "        for i, stance in enumerate(y_predicted):\n",
    "            if stance != 0:\n",
    "                y_predicted[i] = y2_predicted[i]\n",
    "\n",
    "        \n",
    "\n",
    "        precision_scores.append(precision(y_test, y_predicted, model1._stance_map))\n",
    "        recall_scores.append(recall(y_test, y_predicted, model1._stance_map))\n",
    "        accuracy_scores.append(accuracy(y_test, y_predicted, model1._stance_map))\n",
    "\n",
    "        y_test= map_stances(y_test)\n",
    "        y_predicted = map_stances(y_predicted)\n",
    "        competition_score = report_score(y_test, y_predicted)\n",
    "        competition_scores.append(competition_score)\n",
    "        k+=1\n",
    "\n",
    "    print('\\nKfold precision averages: ', score_average(precision_scores, model1))\n",
    "    print('Kfold recall averages: ', score_average(recall_scores, model1))\n",
    "    print ('Kfold accuracy averages: ', score_average(accuracy_scores, model1))\n",
    "    print ('competition score averages: ', sum(competition_scores) / len(competition_scores))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_average(scores, model1):\n",
    "    \n",
    "    score_sums = {stance: 0 for stance in model1._stance_map.items()}\n",
    "    invalid_counts = {stance: 0 for stance in\n",
    "                      model1._stance_map.items()}  # Count number of zero division errors and exclude from averages\n",
    "\n",
    "    for result in scores:\n",
    "        for stance in model1._stance_map.items():\n",
    "            if result[stance] != None:\n",
    "                score_sums[stance] += result[stance]\n",
    "            else:\n",
    "                invalid_counts[stance] += 1\n",
    "\n",
    "    return {stance: score_sums[stance]/(len(scores) - invalid_counts[stance]) for stance in model1._stance_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unrelated 0\n",
      "discuss 1\n",
      "agree 2\n",
      "disagree 3\n"
     ]
    }
   ],
   "source": [
    "for stance, index in  model1._stance_map.items():\n",
    "    print(stance,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_system(X1_features, X2_features, doStratify, numFolds, model1_type, model2_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
