{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, logging, re, nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import feature_extraction\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "class FeatureData(object):\n",
    "    def __init__(self, article_file_path, stances_file_path):\n",
    "        self.number_of_classes = 4\n",
    "        self.classes = ['agree', 'disagree', 'discuss', 'unrelated']\n",
    "        self.articles = self._get_articles(article_file_path)  # list of dictionaries\n",
    "        self.stances = self._get_stances(stances_file_path)\n",
    "        self.number_of_stances = len(self.stances)\n",
    "        self.number_of_articles = len(self.articles)\n",
    "\n",
    "    def get_clean_articles(self):\n",
    "        \"\"\"Returns a dictionary with Body ID's as keys and article bodies as values.\"\"\"\n",
    "        clean_articles = []\n",
    "        print('Retrieving clean articles...')\n",
    "\n",
    "        for item in tqdm.tqdm(self.articles):\n",
    "            cleaned_article = clean(item['articleBody'])\n",
    "            tokens = tokenize_text(cleaned_article)\n",
    "            no_stop_word_tokens = remove_stopwords(tokens)\n",
    "            lemmatized_tokens = get_tokenized_lemmas(no_stop_word_tokens)\n",
    "            clean_articles.append({'articleBody': ' '.join(lemmatized_tokens),\n",
    "                                   'Body ID': item['Body ID']})\n",
    "        return {article['Body ID']: article['articleBody'] for article in clean_articles}\n",
    "\n",
    "    #We need the stop words for POS tagging to work propperly\n",
    "    def get_original_articles(self):\n",
    "        clean_articles = []\n",
    "        print('Retrieving original articles...')\n",
    "        for item in tqdm.tqdm(self.articles):\n",
    "            #cleaned_article = clean(item['articleBody'])\n",
    "            cleaned_article = item['articleBody'].encode('ascii', 'ignore')\n",
    "            clean_articles.append({'articleBody':cleaned_article,\n",
    "                                   'Body ID': item['Body ID']})\n",
    "        return {article['Body ID']: article['articleBody'] for article in clean_articles}\n",
    "\n",
    "    def get_clean_stances(self):\n",
    "        \"\"\"Retrieves a list of dictionaries containing the fully cleaned Headlines and the Body ID and Stance for\n",
    "        each headline.\"\"\"\n",
    "        clean_headlines = []\n",
    "        print('Retrieving clean stances...')\n",
    "\n",
    "        for item in tqdm.tqdm(self.stances):\n",
    "            cleaned_headline = clean(item['Headline'])\n",
    "            tokens = tokenize_text(cleaned_headline)\n",
    "            no_stop_word_tokens = remove_stopwords(tokens)\n",
    "            lemmatized_tokens = get_tokenized_lemmas(no_stop_word_tokens)\n",
    "            clean_headlines.append({'Headline': ' '.join(lemmatized_tokens),\n",
    "                                    'originalHeadline': cleaned_headline,\n",
    "                                    'Body ID': item['Body ID'],\n",
    "                                    'Stance': item['Stance']})\n",
    "\n",
    "        return clean_headlines\n",
    "\n",
    "    def _get_articles(self, path):\n",
    "        # Body ID, articleBody\n",
    "        articles = []\n",
    "        dt=pd.read_csv(path)\n",
    "        articles=dt.T.to_dict().values()\n",
    "        return articles\n",
    "\n",
    "    def _get_stances(self, path):\n",
    "        # Headline, Body ID, Stance\n",
    "        stances = []\n",
    "        dt1=pd.read_csv(path)\n",
    "        stances=dt1.T.to_dict().values()\n",
    "        return stances\n",
    "\n",
    "\n",
    "def normalize_word(w):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(w).lower()\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    return \" \".join(re.findall(r'\\w+', text, flags=re.UNICODE)).lower()\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return [token for token in word_tokenize(text)]\n",
    "\n",
    "\n",
    "def remove_stopwords(list_of_tokens):\n",
    "    return [word for word in list_of_tokens if word not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
    "\n",
    "\n",
    "def get_tokenized_lemmas(tokens):\n",
    "    return [normalize_word(token) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = FeatureData('C:/Users/binni/Downloads/NLP-Fake-News-Challenge-master/NLP-Fake-News-Challenge-master/data/competition_test_bodies.csv', 'C:/Users/binni/Downloads/NLP-Fake-News-Challenge-master/NLP-Fake-News-Challenge-master/data/competition_test_stances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\binni\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "from gensim.models.phrases import Phraser\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_max_ngram_size=3\n",
    "_refuting_words = ['fake', 'fraud', 'hoax', 'false', 'deny', 'denies', 'not', 'despite', 'nope', 'doubt','doubts', 'bogus', 'debunk', 'pranks', 'retract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving clean stances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 25413/25413 [00:56<00:00, 446.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving clean articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 904/904 [00:19<00:00, 18.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving original articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 904/904 [00:00<00:00, 32284.48it/s]\n"
     ]
    }
   ],
   "source": [
    "_stances=feature_data.get_clean_stances()\n",
    "_articles=feature_data.get_clean_articles()\n",
    "_original_articles=feature_data.get_original_articles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "for stance in tqdm.tqdm(_stances):\n",
    "    stance_vectorizer = CountVectorizer(input=stance['Headline'], ngram_range=(1, _max_ngram_size),binary=True)\n",
    "    stance_vectorizer.fit_transform([stance['Headline']]).toarray()\n",
    "    vocab = stance_vectorizer.get_feature_names()\n",
    "    vectorizer = CountVectorizer(input=_articles[stance['Body ID']], vocabulary=vocab,ngram_range=(1, _max_ngram_size))\n",
    "    ngram_counts = vectorizer.fit_transform([_articles[stance['Body ID']]]).toarray()\n",
    "    features = vectorizer.get_feature_names()\n",
    "    aggregated_counts = [0 for _ in range(_max_ngram_size)]\n",
    "    for index in np.nditer(np.nonzero(ngram_counts[0]), ['zerosize_ok']):\n",
    "        aggregated_counts[len(features[index].split()) - 1] += ngram_counts[0][index]\n",
    "    standardized_counts = [1.0*count/len(stance['Headline'].split()) for count in aggregated_counts]\n",
    "    ngrams.append(standardized_counts)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_original_articles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=[]\n",
    "if True:\n",
    "    print('Retrieving headline ngrams...')\n",
    "    ngrams = np.array(ngrams)\n",
    "    features.append(ngrams)\n",
    "    ngram_headings = [('ngram_' + str(count)) for count in range(1,_max_ngram_size + 1)]\n",
    "    feature_names.append(ngram_headings)\n",
    "    print(feature_names)\n",
    "    print(ngram_headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 25413/25413 [05:08<00:00, 82.29it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "all_words = []; atricle_words = []\n",
    "for stance in tqdm.tqdm(_stances):\n",
    "    if stance['Stance'] == 'unrelated':\n",
    "        pass\n",
    "    body_words = []; headline_words = []\n",
    "    headline = tokenizer.tokenize(stance['originalHeadline'])\n",
    "    body = (tokenizer.tokenize(_original_articles[stance['Body ID']].decode('utf-8')))[:4]\n",
    "    for s in headline:\n",
    "        s = word_tokenize(s)\n",
    "        headline_words = headline_words + s\n",
    "        all_words.append(s)\n",
    "    for s in body:\n",
    "        s = word_tokenize(s)\n",
    "        body_words = body_words + s\n",
    "        all_words.append(s)\n",
    "    atricle_words.append([headline_words, body_words])\n",
    "\n",
    "model = models.Word2Vec(all_words, size=100, min_count=1)\n",
    "cosine_similarities = []\n",
    "        # Generate sentence vectors and computer cosine similarity\n",
    "for headline, body in atricle_words:\n",
    "    h_vector = sum([model.wv[word] for word in headline])\n",
    "    b_vector = sum([model.wv[word] for word in body])\n",
    "    cosine_similarities.append(cosine_similarity(h_vector.reshape(1,-1), b_vector.reshape(1,-1)))\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 25413/25413 [00:00<00:00, 55447.77it/s]\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "i=0\n",
    "for stance in tqdm.tqdm(_stances):\n",
    "    count = [1 if refute_word in stance['Headline'] else 0 for refute_word in _refuting_words]\n",
    "    features.append(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 25413/25413 [03:46<00:00, 53.81it/s]\n"
     ]
    }
   ],
   "source": [
    "_refuting_words = ['fake', 'fraud', 'hoax', 'false', 'deny', 'denies', 'not','despite', 'nope', 'nowhere', 'doubt', 'doubts', 'bogus', 'debunk', 'pranks','retract', 'nothing', 'never', 'none', 'budge']\n",
    "\n",
    "def determine_polarity(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    return sum([token in _refuting_words for token in tokens]) % 2\n",
    "\n",
    "polarities = []\n",
    "for stance in tqdm.tqdm(_stances):\n",
    "    headline_polarity = determine_polarity(stance['Headline'])\n",
    "\n",
    "    body_polarity = determine_polarity(_articles.get(stance['Body ID']))\n",
    "\n",
    "    polarities.append([headline_polarity, body_polarity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                | 0/25413 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Headline': 'ferguson riot pregnant woman loses eye cop bean bag round car window', 'originalHeadline': 'ferguson riots pregnant woman loses eye after cops fire bean bag round through car window', 'Body ID': 2008, 'Stance': 'unrelated'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pos_tag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b8d51ff238b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mstance_cosine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mhead\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'originalHeadline'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_original_articles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Body ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-b8d51ff238b1>\u001b[0m in \u001b[0;36mget_tags\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfilter_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamed_tags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnamed_tags\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pos_tag' is not defined"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def get_tags(text):\n",
    "    return pos_tag(word_tokenize(text.encode('ascii', 'ignore')))\n",
    "def filter_pos(named_tags, tag):\n",
    "    return \" \".join([stemmer.stem(name[0]) for name in named_tags if name[1].startswith(tag)])\n",
    "named_cosine = []\n",
    "tags = [\"NN\"]\n",
    "for stance in tqdm.tqdm(_stances):\n",
    "    print(stance)\n",
    "    stance_cosine = []\n",
    "    head = get_tags(stance['originalHeadline'])\n",
    "    print(head)\n",
    "    body = get_tags(_original_articles.get(stance['Body ID'])[:255])\n",
    "    print(body)\n",
    "    print(\"tags\",tags)\n",
    "    for tag in tags:\n",
    "        head_f = filter_pos(head, tag)\n",
    "        body_f = filter_pos(body, tag)\n",
    "        print(\"hf\",head_f)\n",
    "        print(\"bf\",body_f)\n",
    "        if head_f and body_f:\n",
    "            vect = TfidfVectorizer(min_df=1)\n",
    "            tfidf = vect.fit_transform([head_f,body_f])\n",
    "            print(tfidf)\n",
    "            cosine = (tfidf * tfidf.T).todense().tolist()\n",
    "            print(cosine)\n",
    "            if len(cosine) == 2:\n",
    "                stance_cosine.append(cosine[1][0])\n",
    "            else:\n",
    "                stance_cosine.append(0)\n",
    "        else:\n",
    "            stance_cosine.append(0)\n",
    "        print(stance_cosine)\n",
    "    named_cosine.append(stance_cosine)\n",
    "    print(named_cosine)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
