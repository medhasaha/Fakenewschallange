{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, logging, re, nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import feature_extraction\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "class FeatureData(object):\n",
    "    def __init__(self, article_file_path, stances_file_path):\n",
    "        self.number_of_classes = 4\n",
    "        self.classes = ['agree', 'disagree', 'discuss', 'unrelated']\n",
    "        self.articles = self._get_articles(article_file_path)  # list of dictionaries\n",
    "        self.stances = self._get_stances(stances_file_path)\n",
    "        self.number_of_stances = len(self.stances)\n",
    "        self.number_of_articles = len(self.articles)\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_clean_articles(self):\n",
    "        \"\"\"Returns a dictionary with Body ID's as keys and article bodies as values.\"\"\"\n",
    "        clean_articles = []\n",
    "        print('Retrieving clean articles...')\n",
    "\n",
    "        for item in tqdm.tqdm(self.articles):\n",
    "            cleaned_article = clean(item['articleBody'])\n",
    "            tokens = tokenize_text(cleaned_article)\n",
    "            no_stop_word_tokens = remove_stopwords(tokens)\n",
    "            lemmatized_tokens = get_tokenized_lemmas(no_stop_word_tokens)\n",
    "            clean_articles.append({'articleBody': ' '.join(lemmatized_tokens),\n",
    "                                   'Body ID': item['Body ID']})\n",
    "        return {article['Body ID']: article['articleBody'] for article in clean_articles}\n",
    "\n",
    "    #We need the stop words for POS tagging to work propperly\n",
    "    def get_original_articles(self):\n",
    "        clean_articles = []\n",
    "        print('Retrieving original articles...')\n",
    "        for item in tqdm.tqdm(self.articles):\n",
    "            #cleaned_article = clean(item['articleBody'])\n",
    "            cleaned_article = item['articleBody'].encode('ascii', 'ignore')\n",
    "            clean_articles.append({'articleBody':cleaned_article,\n",
    "                                   'Body ID': item['Body ID']})\n",
    "        return {article['Body ID']: article['articleBody'] for article in clean_articles}\n",
    "\n",
    "    def get_clean_stances(self):\n",
    "        \"\"\"Retrieves a list of dictionaries containing the fully cleaned Headlines and the Body ID and Stance for\n",
    "        each headline.\"\"\"\n",
    "        clean_headlines = []\n",
    "        print('Retrieving clean stances...')\n",
    "\n",
    "        for item in tqdm.tqdm(self.stances):\n",
    "            cleaned_headline = clean(item['Headline'])\n",
    "            tokens = tokenize_text(cleaned_headline)\n",
    "            no_stop_word_tokens = remove_stopwords(tokens)\n",
    "            lemmatized_tokens = get_tokenized_lemmas(no_stop_word_tokens)\n",
    "            clean_headlines.append({'Headline': ' '.join(lemmatized_tokens),\n",
    "                                    'originalHeadline': cleaned_headline,\n",
    "                                    'Body ID': item['Body ID'],\n",
    "                                    'Stance': item['Stance']})\n",
    "\n",
    "        return clean_headlines\n",
    "\n",
    "    def _get_articles(self, path):\n",
    "        # Body ID, articleBody\n",
    "        articles = []\n",
    "        dt=pd.read_csv(path)\n",
    "        articles=dt.T.to_dict().values()\n",
    "        return articles\n",
    "\n",
    "    def _get_stances(self, path):\n",
    "        # Headline, Body ID, Stance\n",
    "        stances = []\n",
    "        dt1=pd.read_csv(path)\n",
    "        stances=dt1.T.to_dict().values()\n",
    "        return stances\n",
    "\n",
    "\n",
    "def normalize_word(w):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(w).lower()\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    return \" \".join(re.findall(r'\\w+', text, flags=re.UNICODE)).lower()\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return [token for token in word_tokenize(text)]\n",
    "\n",
    "\n",
    "def remove_stopwords(list_of_tokens):\n",
    "    return [word for word in list_of_tokens if word not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
    "\n",
    "\n",
    "def get_tokenized_lemmas(tokens):\n",
    "    return [normalize_word(token) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\binni\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\binni\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk, sent_tokenize\n",
    "from gensim import models\n",
    "from gensim.models.phrases import Phraser\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.chunk import tree2conlltags\n",
    "from textacy.doc import Doc\n",
    "from textacy.extract import direct_quotations\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureGenerator(object):\n",
    "    \"\"\"Class responsible for generating each feature used in the X matrix.\"\"\"\n",
    "\n",
    "    def __init__(self, clean_articles, clean_stances, original_articles, load_data=True):\n",
    "        self._articles = clean_articles  # dictionary {article ID: body}\n",
    "        self._original_articles = original_articles\n",
    "        self._stances = clean_stances  # list of dictionaries\n",
    "        self._max_ngram_size = 3\n",
    "        self._refuting_words = ['fake', 'fraud', 'hoax', 'false', 'deny', 'denies', 'not', 'despite', 'nope', 'doubt',\n",
    "                                'doubts', 'bogus', 'debunk', 'pranks', 'retract']\n",
    "    \n",
    "    def get_features(self, features_directory=\"features\"):\n",
    "        \"\"\"Retrieves the full set of features as a matrix (the X matrix for training). You only need to run this\n",
    "        if the features have been updated since the last time they were output to a file under the features\n",
    "        directory.\"\"\"\n",
    "        feature_names = []\n",
    "        features = []\n",
    "        \"\"\"\"if True:\n",
    "            print('Retrieving headline ngrams...')\n",
    "            ngrams = np.array(self._get_ngrams())\n",
    "            features.append(ngrams)\n",
    "            ngram_headings = [('ngram_' + str(count)) for count in range(1, self._max_ngram_size + 1)]\n",
    "            feature_names.append(ngram_headings)\n",
    "            self._feature_to_csv(ngrams, ngram_headings,\"C:/Users/binni/minor/\"+features_directory+'/ngrams.csv')\n",
    "\n",
    "        if True:\n",
    "            print('Retrieving word2Vec...')\n",
    "            word2Vec = np.array(self._get_word2vec()).reshape(len(self._stances), 1)\n",
    "            features.append(word2Vec)\n",
    "            feature_names.append(\"word2Vec\")\n",
    "            self._feature_to_csv(word2Vec, [\"word2Vec\"], \"C:/Users/binni/minor/\"+features_directory + '/word2Vec.csv')\n",
    "\n",
    "        if True:\n",
    "            print('Retrieving refuting words...')\n",
    "            refuting = np.array(self._get_refuting_words())\n",
    "            features.append(refuting)\n",
    "            [feature_names.append(word + '_refuting') for word in self._refuting_words]\n",
    "            self._feature_to_csv(refuting, self._refuting_words, \"C:/Users/binni/minor/\"+features_directory+'/refuting.csv')\n",
    "\n",
    "        if True:\n",
    "            print('Retrieving polarity...')\n",
    "            polarity = np.array(self._polarity_feature())\n",
    "            features.append(polarity)\n",
    "            feature_names.append('headline_polarity')\n",
    "            feature_names.append('article_polarity')\n",
    "            self._feature_to_csv(polarity, ['headline_polarity', 'article_polarity'],\"C:/Users/binni/minor/\"+ features_directory+'/polarity.csv')\n",
    "        if True:\n",
    "            print('Retrieving named entity cosine...')\n",
    "            named_cosine = np.array(self._named_entity_feature()).reshape(len(self._stances), 1)\n",
    "            features.append(named_cosine)\n",
    "            feature_names.append('named_cosine')\n",
    "            self._feature_to_csv(named_cosine, ['named_cosine'],\"C:/Users/binni/minor/\"+ features_directory+'/named_cosine.csv')\"\"\"\n",
    "\n",
    "        if True:\n",
    "            print('Retrieving VADER...')\n",
    "            vader = np.array(self._vader_feature()).reshape(len(self._stances), 2)\n",
    "            features.append(vader)\n",
    "            feature_names.append('vader_pos')\n",
    "            feature_names.append('vader_neg')\n",
    "            self._feature_to_csv(vader, ['vader'],\"C:/Users/binni/minor/\"+ features_directory+'/vader.csv')\n",
    "\n",
    "        if True:\n",
    "            print('Retrieving jaccard similarities...')\n",
    "            jaccard = np.array(self._get_jaccard_similarity()).reshape(len(self._stances), 1)\n",
    "            features.append(jaccard)\n",
    "            feature_names.append('jaccard_similarity')\n",
    "            self._feature_to_csv(jaccard, ['jaccard_similarity'],\"C:/Users/binni/minor/\"+ features_directory+'/jaccard_similarity.csv')\n",
    "\n",
    "        if True:\n",
    "            print('Retrieving quote analysis...')\n",
    "            quotes = np.array(self._get_quotes()).reshape(len(self._stances), 1)\n",
    "            features.append(quotes)\n",
    "            feature_names.append('quote_analysis')\n",
    "            self._feature_to_csv(quotes, ['quote_analysis'],\"C:/Users/binni/minor/\"+features_directory+'/quote_analysis.csv')\n",
    "\n",
    "        if True:\n",
    "            lengths = np.array(self._length_feature()).reshape(len(self._stances), 1)\n",
    "            features.append(lengths)\n",
    "            feature_names.append('lengths')\n",
    "            self._feature_to_csv(lengths, ['lengths'], \"C:/Users/binni/minor/\"+features_directory + '/lengths.csv')\n",
    "\n",
    "        if True:\n",
    "            logging.debug('Retrieving punctuation frequencies...')\n",
    "            punctuation_frequencies = np.array(self._get_punctuation_frequency()).reshape(len(self._stances), 1)\n",
    "            features.append(punctuation_frequencies)\n",
    "            feature_names.append('punctuation_frequency')\n",
    "            self._feature_to_csv(punctuation_frequencies, ['punctuation_frequency'],\"C:/Users/binni/minor/\"+features_directory + '/punctuation_frequency')\n",
    "\n",
    "        return {'feature_matrix': np.concatenate(features, axis=1), 'feature_names': feature_names}\n",
    "\n",
    "    def _feature_to_csv(self, feature, feature_headers, output_path):\n",
    "        \"\"\"Outputs a feature to a csv file. feature is a 2d numpy matrix containing the feature values and\n",
    "        feature headers is a list containing the feature's column headings.\"\"\"\n",
    "        header = ','.join(feature_headers)\n",
    "        np.savetxt(fname=output_path, X=feature, delimiter=',', header=header, comments='')\n",
    "\n",
    "    def _get_ngrams(self):\n",
    "        \"\"\"Retrieves counts for ngrams of the article title in the article itself, from one up to size max_ngram_size.\n",
    "        Returns a list of lists, each containing the counts for a different size of ngram.\"\"\"\n",
    "        ngrams = []\n",
    "\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            # Retrieves the vocabulary of ngrams for the headline.\n",
    "            stance_vectorizer = CountVectorizer(input=stance['Headline'], ngram_range=(1, self._max_ngram_size),\n",
    "                                                binary=True)\n",
    "            stance_vectorizer.fit_transform([stance['Headline']]).toarray()\n",
    "\n",
    "            # Search the article text and count headline ngrams.\n",
    "            vocab = stance_vectorizer.get_feature_names()\n",
    "            vectorizer = CountVectorizer(input=self._articles[stance['Body ID']], vocabulary=vocab,\n",
    "                                         ngram_range=(1, self._max_ngram_size))\n",
    "            ngram_counts = vectorizer.fit_transform([self._articles[stance['Body ID']]]).toarray()\n",
    "            features = vectorizer.get_feature_names()\n",
    "\n",
    "            aggregated_counts = [0 for _ in range(self._max_ngram_size)]\n",
    "\n",
    "            # Create a list of the aggregated counts of each ngram size.\n",
    "            for index in np.nditer(np.nonzero(ngram_counts[0]), ['zerosize_ok']):\n",
    "                aggregated_counts[len(features[index].split()) - 1] += ngram_counts[0][index]\n",
    "\n",
    "            # attempt to standardize ngram counts across headlines and bodies of varying length by dividing total\n",
    "            # ngram hits by the length of the headline. These will need to be normalized later so they lie\n",
    "            # between 0 and 1.\n",
    "            standardized_counts = [1.0*count/len(stance['Headline'].split()) for count in aggregated_counts]\n",
    "\n",
    "            ngrams.append(standardized_counts)\n",
    "            #print ngrams\n",
    "\n",
    "        return ngrams\n",
    "\n",
    "    def _get_word2vec(self):\n",
    "        # Gather sentences\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        all_words = []; atricle_words = []\n",
    "\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            if stance['Stance'] == 'unrelated':\n",
    "                pass\n",
    "            body_words = []; headline_words = []\n",
    "            headline = tokenizer.tokenize(stance['originalHeadline'])\n",
    "            body = (tokenizer.tokenize(self._original_articles[stance['Body ID']].decode('utf-8')))[:4]\n",
    "            for s in headline:\n",
    "                s = word_tokenize(s)\n",
    "                headline_words = headline_words + s\n",
    "                all_words.append(s)\n",
    "            for s in body:\n",
    "                s = word_tokenize(s)\n",
    "                body_words = body_words + s\n",
    "                all_words.append(s)\n",
    "            atricle_words.append([headline_words, body_words])\n",
    "\n",
    "        # Train Word2Vec\n",
    "        model = models.Word2Vec(all_words, size=100, min_count=1)\n",
    "\n",
    "        cosine_similarities = []\n",
    "        # Generate sentence vectors and computer cosine similarity\n",
    "        for headline, body in atricle_words:\n",
    "            h_vector = sum([model.wv[word] for word in headline])\n",
    "            b_vector = sum([model.wv[word] for word in body])\n",
    "            cosine_similarities.append(cosine_similarity(h_vector.reshape(1,-1), b_vector.reshape(1,-1)))\n",
    "\n",
    "        return cosine_similarities\n",
    "\n",
    "    def _get_refuting_words(self):\n",
    "        \"\"\" Retrieves headlines of the articles and indicates a count of each of the refuting words in the headline.\n",
    "        Returns a list containing the number of refuting words found (at lease once) in the headline. \"\"\"\n",
    "\n",
    "        features = []\n",
    "\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            # print \"[DEBUG] stance \", stance\n",
    "            count = [1 if refute_word in stance['Headline'] else 0 for refute_word in self._refuting_words]\n",
    "            # print \"[DEBUG] count \", count\n",
    "            features.append(count)\n",
    "        # print \"[DEBUG] features\", features\n",
    "        return features\n",
    "\n",
    "    def _polarity_feature(self):\n",
    "        _refuting_words = ['fake', 'fraud', 'hoax', 'false', 'deny', 'denies', 'not',\n",
    "                           'despite', 'nope', 'nowhere', 'doubt', 'doubts', 'bogus', 'debunk', 'pranks',\n",
    "                           'retract', 'nothing', 'never', 'none', 'budge']\n",
    "\n",
    "        def determine_polarity(text):\n",
    "            tokens = tokenize_text(text)\n",
    "            return sum([token in _refuting_words for token in tokens]) % 2\n",
    "\n",
    "        polarities = []\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            headline_polarity = determine_polarity(stance['Headline'])\n",
    "            body_polarity = determine_polarity(self._articles.get(stance['Body ID']))\n",
    "            polarities.append([headline_polarity, body_polarity])\n",
    "\n",
    "        return polarities\n",
    "\n",
    "    def _named_entity_feature(self):\n",
    "        \"\"\" Retrieves a list of Named Entities from the Headline and Body.\n",
    "        Returns a list containing the cosine similarity between the counts of the named entities \"\"\"\n",
    "        stemmer = PorterStemmer()\n",
    "        def get_tags(text):\n",
    "            return pos_tag(word_tokenize(text))\n",
    "\n",
    "        def filter_pos(named_tags, tag):\n",
    "            return \" \".join([stemmer.stem(name[0]) for name in named_tags if name[1].startswith(tag)])\n",
    "\n",
    "        named_cosine = []\n",
    "        tags = [\"NN\"]\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            stance_cosine = []\n",
    "            head = get_tags(stance['originalHeadline'])\n",
    "            body = get_tags(self._original_articles.get(stance['Body ID']).decode('utf-8')[:255])\n",
    "\n",
    "            for tag in tags:\n",
    "                head_f = filter_pos(head, tag)\n",
    "                body_f = filter_pos(body, tag)\n",
    "\n",
    "                if head_f and body_f:\n",
    "                    vect = TfidfVectorizer(min_df=1)\n",
    "                    tfidf = vect.fit_transform([head_f,body_f])\n",
    "                    cosine = (tfidf * tfidf.T).todense().tolist()\n",
    "                    if len(cosine) == 2:\n",
    "                        stance_cosine.append(cosine[1][0])\n",
    "                    else:\n",
    "                        stance_cosine.append(0)\n",
    "                else:\n",
    "                    stance_cosine.append(0)\n",
    "            named_cosine.append(stance_cosine)\n",
    "        return named_cosine\n",
    "\n",
    "    def _vader_feature(self):\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        features = []\n",
    "\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            headVader = sid.polarity_scores(stance[\"Headline\"])\n",
    "            bodyVader = sid.polarity_scores(sent_tokenize(self._original_articles.get(stance['Body ID']).decode('utf-8'))[0])\n",
    "            features.append(abs(headVader['pos']-bodyVader['pos']))\n",
    "            features.append(abs(headVader['neg']-bodyVader['neg']))\n",
    "        return features\n",
    "\n",
    "    def _get_jaccard_similarity(self):\n",
    "        \"\"\" Get the jaccard similarities for each headline and article body pair. Jaccard similarity is defined as\n",
    "        J(A, B) = |A intersect B| / |A union B|. Try to normalize by only considering the first\"\"\"\n",
    "        similarities = []\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            headline = set(stance['Headline'].split())\n",
    "            body = set(self._articles.get(stance['Body ID']).split()[:255])\n",
    "            jaccard = float(len(headline.intersection(body))) / len(headline.union(body))\n",
    "            similarities.append(jaccard)\n",
    "\n",
    "        return similarities\n",
    "\n",
    "    def _get_quotes(self):\n",
    "        quote_count = []\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            body = self._original_articles.get(stance['Body ID']).decode('utf-8', 'replace')\n",
    "            doc = Doc(content=body, lang='en')\n",
    "            quotes = direct_quotations(doc)\n",
    "            quote_counter = 0\n",
    "\n",
    "            for q in quotes:\n",
    "                quote_counter = quote_counter + len(q[2])\n",
    "            quote_counter = quote_counter / len(body)\n",
    "            quote_count.append(quote_counter)\n",
    "\n",
    "        return quote_count\n",
    "\n",
    "    def _length_feature(self):\n",
    "        lengths = []\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            lengths.append(len(self._original_articles.get(stance['Body ID'])))\n",
    "        return lengths\n",
    "\n",
    "    def _get_punctuation_frequency(self):\n",
    "        frequencies = []\n",
    "\n",
    "        for stance in tqdm.tqdm(self._stances):\n",
    "            question_marks = 0\n",
    "            exclamation_marks = 0\n",
    "            article_body = self._original_articles[stance['Body ID']]\n",
    "\n",
    "            for character in article_body:\n",
    "                if character == '?':\n",
    "                    question_marks += 1\n",
    "                elif character == '!':\n",
    "                    exclamation_marks += 1\n",
    "\n",
    "            frequency = (question_marks + exclamation_marks) / len(article_body.split())\n",
    "            frequencies.append(frequency)\n",
    "\n",
    "        return frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving clean articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1683/1683 [00:42<00:00, 39.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving clean stances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 49972/49972 [01:12<00:00, 690.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving original articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 1683/1683 [00:00<00:00, 22364.41it/s]\n"
     ]
    }
   ],
   "source": [
    "feature_data = FeatureData('C:\\\\Users\\\\binni\\\\minor\\\\data\\\\train_bodies.csv', 'C:\\\\Users\\\\binni\\\\minor\\\\data\\\\train_stances.csv')\n",
    "feature_generator = FeatureGenerator(feature_data.get_clean_articles(), feature_data.get_clean_stances(), feature_data.get_original_articles())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving VADER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 49972/49972 [07:07<00:00, 116.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving jaccard similarities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 49972/49972 [00:08<00:00, 6065.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving quote analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                | 0/49972 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e4681d4b7690>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"feature_generator\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-ef7be1a0a25f>\u001b[0m in \u001b[0;36mget_features\u001b[1;34m(self, features_directory)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Retrieving quote analysis...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mquotes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_quotes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquotes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mfeature_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'quote_analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ef7be1a0a25f>\u001b[0m in \u001b[0;36m_get_quotes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_articles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Body ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'replace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m             \u001b[0mquotes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirect_quotations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[0mquote_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textacy\\doc.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, content, metadata, lang)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect_language\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0municode_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSpacyDoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_from_spacy_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textacy\\doc.py\u001b[0m in \u001b[0;36m_init_from_text\u001b[1;34m(self, content, metadata, lang)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mlangstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy_lang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0municode_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mspacy_lang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_spacy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m             \u001b[0mlangstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy_lang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\cachetools\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                     \u001b[1;32mpass\u001b[0m  \u001b[1;31m# key not found\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                     \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textacy\\cache.py\u001b[0m in \u001b[0;36mload_spacy\u001b[1;34m(name, disable)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mdisable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0mLOGGER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loading \"%s\" spaCy pipeline'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'exists'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "features = feature_generator.get_features(\"feature_generator\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
